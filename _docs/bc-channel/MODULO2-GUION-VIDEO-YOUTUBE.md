# Bootcamp Bash - MÃ³dulo 2: Desarrollo Paso a Paso

## GuiÃ³n para Video de YouTube - Comandos Avanzados y Pipes

---

## ðŸ“‹ INFORMACIÃ“N DEL VIDEO

**TÃ­tulo:** "Bootcamp Bash - MÃ³dulo 2: Comandos Avanzados y Pipes - Dominando el Procesamiento de Texto"

**DuraciÃ³n Estimada:** 50-65 minutos

**Audiencia:** Estudiantes que completaron el MÃ³dulo 1 y quieren dominar el procesamiento de texto en Bash

**Objetivo:** Dominar pipes, redirecciÃ³n y herramientas de procesamiento de texto para crear flujos de trabajo potentes

---

## ðŸŽ¯ OBJETIVOS DE APRENDIZAJE

Al finalizar este video, los estudiantes podrÃ¡n:

1. **Dominar** el uso de pipes para encadenar comandos
2. **Implementar** redirecciÃ³n avanzada de entrada y salida
3. **Procesar** texto eficientemente con grep, sed, awk
4. **Utilizar** wildcards y pattern matching avanzado
5. **Crear** filtros complejos de datos
6. **Desarrollar** un procesador de logs funcional

---

## ðŸ“ ESTRUCTURA DEL VIDEO

### PARTE 1: INTRODUCCIÃ“N Y REPASO (5 minutos)

### PARTE 2: PIPES Y REDIRECCIÃ“N AVANZADA (15 minutos)

### PARTE 3: MAESTRÃA EN GREP Y EXPRESIONES REGULARES (15 minutos)

### PARTE 4: SED - EL EDITOR DE FLUJO (10 minutos)

### PARTE 5: AWK - PROCESAMIENTO ESTRUCTURADO (10 minutos)

### PARTE 6: PROYECTO PRÃCTICO - PROCESADOR DE LOGS (10 minutos)

---

## ðŸŽ¬ PARTE 1: INTRODUCCIÃ“N Y REPASO (5 minutos)

### ðŸŽ¤ GuiÃ³n de PresentaciÃ³n

**[PANTALLA: Logo del MÃ³dulo 2 con SVG]**

> "Â¡Bienvenidos de vuelta al Bootcamp Bash! Soy tu instructor y hoy vamos a elevar tus habilidades al siguiente nivel con el MÃ³dulo 2: Comandos Avanzados y Pipes."

**[TRANSICIÃ“N: Mostrar logros del MÃ³dulo 1]**

> "En el mÃ³dulo anterior aprendimos los fundamentos: navegaciÃ³n, comandos bÃ¡sicos, nuestro primer script. Hoy vamos a aprender a combinar estos comandos para crear flujos de trabajo realmente potentes."

### ðŸ“Š El Poder de los Pipes

**[PANTALLA: Diagrama conceptual de pipes]**

> "Los pipes son como tuberÃ­as que conectan comandos. La salida de un comando se convierte en la entrada del siguiente, creando cadenas de procesamiento increÃ­blemente eficientes."

**[PANTALLA: Ejemplo visual]**

> "Imagina procesar un archivo de 10GB de logs en segundos, extraer informaciÃ³n especÃ­fica, formatearla y generar un reporte. Eso es el poder que vamos a dominar hoy."

### ðŸ—ºï¸ Roadmap del MÃ³dulo 2

**[PANTALLA: Estructura del mÃ³dulo]**

> "En este mÃ³dulo cubriremos:
>
> - Pipes y redirecciÃ³n avanzada
> - Grep con expresiones regulares
> - Sed para ediciÃ³n de flujo
> - Awk para procesamiento estructurado
> - Wildcards y pattern matching
> - Proyecto: Procesador de Logs Avanzado"

### ðŸ”„ Repaso RÃ¡pido

**[PANTALLA: Terminal]**

> "Repasemos rÃ¡pidamente el MÃ³dulo 1 creando algunos archivos de prueba:"

```bash
# Configurar entorno para el mÃ³dulo 2
cd ~/bootcamp-bash
mkdir -p modulo2/{ejercicios,proyectos,datos}
cd modulo2

# Crear datos de ejemplo
cat > datos/empleados.txt << 'EOF'
Juan PÃ©rez,Desarrollador,2500,Madrid,juan@empresa.com
MarÃ­a GarcÃ­a,DiseÃ±adora,2200,Barcelona,maria@empresa.com
Pedro LÃ³pez,Administrador,2800,Valencia,pedro@empresa.com
Ana MartÃ­n,DevOps,3000,Sevilla,ana@empresa.com
Carlos Ruiz,Backend,2600,Bilbao,carlos@empresa.com
Laura Torres,Frontend,2400,MÃ¡laga,laura@empresa.com
EOF

echo "âœ… Datos preparados para el mÃ³dulo 2"
```

---

## ðŸ”— PARTE 2: PIPES Y REDIRECCIÃ“N AVANZADA (15 minutos)

### ðŸŽ¤ TransiciÃ³n

**[PANTALLA: Diagrama de flujo de datos]**

> "Los pipes son la columna vertebral del procesamiento en Bash. Permiten que la salida de un comando se convierta automÃ¡ticamente en la entrada del siguiente."

### ðŸš° Fundamentos de Pipes

**[PANTALLA: DemostraciÃ³n en vivo]**

> "Empezamos con pipes bÃ¡sicos y vamos escalando la complejidad:"

```bash
# Pipe bÃ¡sico
cat datos/empleados.txt | head -3

# Encadenar mÃºltiples comandos
cat datos/empleados.txt | grep "Madrid" | cut -d',' -f1

# Contar elementos
cat datos/empleados.txt | wc -l
ls | wc -l

# Ordenar y mostrar Ãºnicos
echo -e "manzana\nbanana\nmanzana\nnaranja\nbanana" | sort | uniq

# Pipeline complejo
cat datos/empleados.txt | grep -v "DiseÃ±adora" | sort | head -2
```

### ðŸ“¤ RedirecciÃ³n Avanzada

**[PANTALLA: ExplicaciÃ³n de redirecciÃ³n]**

> "La redirecciÃ³n nos permite controlar exactamente dÃ³nde van los datos:"

```bash
# RedirecciÃ³n bÃ¡sica
echo "Log de inicio: $(date)" > logs/sistema.log
echo "Proceso completado" >> logs/sistema.log

# RedirecciÃ³n de errores
ls archivo_inexistente 2> errores.log
ls archivo_inexistente 2>> errores.log

# Redireccionar salida y errores por separado
comando_ejemplo > salida.txt 2> errores.txt

# Redireccionar ambos al mismo archivo
ls /etc /directorio_falso > completo.log 2>&1

# Here documents
cat << 'EOF' > script_generado.sh
#!/bin/bash
echo "Script generado automÃ¡ticamente"
date
EOF
```

### ðŸ”„ Pipes Avanzados y Tee

**[PANTALLA: DemostraciÃ³n de tee]**

> "El comando tee es como una 'T' en fontanerÃ­a - divide el flujo:"

```bash
# Guardar y mostrar simultÃ¡neamente
cat datos/empleados.txt | grep "Madrid" | tee madrid_empleados.txt

# MÃºltiples destinos
echo "Mensaje importante" | tee archivo1.txt archivo2.txt archivo3.txt

# AÃ±adir a archivos existentes
date | tee -a logs/acceso.log logs/general.log

# Pipeline con tee para debugging
cat datos/empleados.txt |
  grep "Desarrollador\|Backend" |
  tee desarrolladores_temp.txt |
  cut -d',' -f1,3 |
  sort -t',' -k2 -n
```

### ðŸ”§ Herramientas de Filtrado

**[PANTALLA: Comandos de filtrado]**

> "Herramientas especÃ­ficas para filtrar y transformar datos:"

```bash
# cut - extraer columnas
cut -d',' -f1,3 datos/empleados.txt  # Nombre y salario
cut -d',' -f1-2 datos/empleados.txt  # Primeras dos columnas

# sort - ordenamiento avanzado
sort datos/empleados.txt                    # Orden alfabÃ©tico
sort -t',' -k3 -n datos/empleados.txt      # Por salario (numÃ©rico)
sort -t',' -k4 datos/empleados.txt         # Por ciudad

# uniq - elementos Ãºnicos
cut -d',' -f4 datos/empleados.txt | sort | uniq  # Ciudades Ãºnicas
cut -d',' -f4 datos/empleados.txt | sort | uniq -c  # Con conteos

# tr - transformar caracteres
cat datos/empleados.txt | tr ',' '\t'       # Comas a tabs
echo "TEXTO EN MAYÃšSCULAS" | tr 'A-Z' 'a-z'  # A minÃºsculas
```

---

## ðŸ” PARTE 3: MAESTRÃA EN GREP Y EXPRESIONES REGULARES (15 minutos)

### ðŸŽ¤ TransiciÃ³n

**[PANTALLA: Logo de grep con ejemplos]**

> "Grep es probablemente la herramienta de bÃºsqueda mÃ¡s potente en Unix. Vamos a dominar desde bÃºsquedas bÃ¡sicas hasta expresiones regulares complejas."

### ðŸŽ¯ Grep BÃ¡sico a Avanzado

**[PANTALLA: DemostraciÃ³n progresiva]**

> "Empezamos con grep bÃ¡sico y escalamos la complejidad:"

```bash
# Crear archivo de logs para practicar
cat > datos/servidor.log << 'EOF'
2024-01-15 08:30:01 INFO Usuario juan@empresa.com conectado
2024-01-15 08:31:15 ERROR Fallo de conexiÃ³n en 192.168.1.100
2024-01-15 08:32:30 INFO Usuario maria@empresa.com conectado
2024-01-15 08:33:45 WARNING Memoria al 85% en servidor web
2024-01-15 08:35:00 ERROR Base de datos no responde
2024-01-15 08:36:12 INFO Usuario pedro@empresa.com desconectado
2024-01-15 08:37:30 INFO Backup completado exitosamente
2024-01-15 08:38:45 ERROR Timeout en API externa
EOF

# BÃºsquedas bÃ¡sicas
grep "ERROR" datos/servidor.log
grep -i "info" datos/servidor.log           # Case insensitive
grep -n "WARNING" datos/servidor.log        # Con nÃºmeros de lÃ­nea
grep -v "INFO" datos/servidor.log           # Excluir lÃ­neas
grep -c "ERROR" datos/servidor.log          # Contar coincidencias
```

### ðŸŽ­ Expresiones Regulares

**[PANTALLA: IntroducciÃ³n a regex]**

> "Las expresiones regulares son patrones que describen texto. Son increÃ­blemente poderosas:"

```bash
# Metacaracteres bÃ¡sicos
grep "^2024" datos/servidor.log             # LÃ­neas que empiezan con 2024
grep "conectado$" datos/servidor.log        # LÃ­neas que terminan con 'conectado'
grep "08:3[0-9]" datos/servidor.log         # Minutos entre 30-39

# Clases de caracteres
grep "[0-9]" datos/servidor.log             # Cualquier dÃ­gito
grep "[A-Z][A-Z][A-Z]" datos/servidor.log   # Tres mayÃºsculas seguidas
grep "[0-9]\{1,3\}\.[0-9]\{1,3\}" datos/servidor.log  # IPs parciales

# Cuantificadores
grep "o\+" datos/servidor.log               # Una o mÃ¡s 'o'
grep "o*" datos/servidor.log                # Cero o mÃ¡s 'o'
grep "o\?" datos/servidor.log               # Cero o una 'o'

# Ejemplos prÃ¡cticos
grep "[a-zA-Z]\+@[a-zA-Z]\+\.[a-zA-Z]\+" datos/servidor.log  # Emails
grep "192\.168\.[0-9]\+\.[0-9]\+" datos/servidor.log        # IPs locales
```

### ðŸ”§ Grep con Opciones Avanzadas

**[PANTALLA: Opciones potentes]**

> "Opciones que multiplican el poder de grep:"

```bash
# BÃºsqueda en mÃºltiples archivos
grep -r "ERROR" logs/                       # Recursivo en directorio
grep -l "ERROR" *.log                       # Solo nombres de archivos
grep -L "SUCCESS" *.log                     # Archivos SIN el patrÃ³n

# Contexto alrededor de coincidencias
grep -A 2 "ERROR" datos/servidor.log        # 2 lÃ­neas despuÃ©s
grep -B 2 "ERROR" datos/servidor.log        # 2 lÃ­neas antes
grep -C 2 "ERROR" datos/servidor.log        # 2 lÃ­neas antes y despuÃ©s

# MÃºltiples patrones
grep -E "ERROR|WARNING" datos/servidor.log   # OR lÃ³gico
grep -F "192.168.1.100" datos/servidor.log   # Literal (sin regex)

# Pipeline con grep
cat datos/empleados.txt | grep "Madrid\|Barcelona" | grep "Desarrollador"
```

### ðŸš€ Casos de Uso Reales

**[PANTALLA: Ejemplos prÃ¡cticos]**

> "Casos reales donde grep es imprescindible:"

```bash
# Analizar logs de Apache
grep "404" /var/log/apache2/access.log | head -10

# Encontrar procesos
ps aux | grep python | grep -v grep

# Buscar configuraciones
grep -r "port.*80" /etc/ 2>/dev/null

# Extraer IPs Ãºnicas de logs
grep -o "[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}" datos/servidor.log | sort | uniq
```

---

## âœï¸ PARTE 4: SED - EL EDITOR DE FLUJO (10 minutos)

### ðŸŽ¤ TransiciÃ³n

**[PANTALLA: Logo conceptual de sed]**

> "Sed significa 'Stream Editor' - editor de flujo. Es como un editor de texto que trabaja con pipes, perfecto para transformaciones automatizadas."

### ðŸ”„ Operaciones BÃ¡sicas de Sed

**[PANTALLA: DemostraciÃ³n en vivo]**

> "Sed trabaja lÃ­nea por lÃ­nea, aplicando transformaciones segÃºn patrones:"

```bash
# Sustituir texto (primera ocurrencia por lÃ­nea)
sed 's/ERROR/FALLO/' datos/servidor.log

# Sustituir todas las ocurrencias
sed 's/ERROR/FALLO/g' datos/servidor.log

# Sustituir solo en lÃ­neas especÃ­ficas
sed '2s/INFO/INFORMACIÃ“N/' datos/servidor.log

# Guardar cambios en archivo
sed 's/ERROR/FALLO/g' datos/servidor.log > datos/servidor_modificado.log

# Modificar archivo in-place (Â¡CUIDADO!)
cp datos/servidor.log datos/servidor_backup.log
sed -i 's/WARNING/ALERTA/g' datos/servidor_backup.log
```

### ðŸŽ¯ Operaciones Avanzadas

**[PANTALLA: Comandos complejos]**

> "Sed puede hacer mucho mÃ¡s que sustituir texto:"

```bash
# Eliminar lÃ­neas
sed '/INFO/d' datos/servidor.log            # Eliminar lÃ­neas con INFO
sed '1d' datos/servidor.log                 # Eliminar primera lÃ­nea
sed '$d' datos/servidor.log                 # Eliminar Ãºltima lÃ­nea
sed '2,4d' datos/servidor.log               # Eliminar lÃ­neas 2-4

# Agregar lÃ­neas
sed '1i\=== INICIO DE LOG ===' datos/servidor.log    # Insertar al principio
sed '$a\=== FIN DE LOG ===' datos/servidor.log       # Agregar al final

# Imprimir lÃ­neas especÃ­ficas
sed -n '2,4p' datos/servidor.log            # Solo lÃ­neas 2-4
sed -n '/ERROR/p' datos/servidor.log        # Solo lÃ­neas con ERROR

# MÃºltiples operaciones
sed -e 's/ERROR/FALLO/g' -e 's/WARNING/ALERTA/g' datos/servidor.log
```

### ðŸ”§ Sed con Expresiones Regulares

**[PANTALLA: Regex en sed]**

> "Combinando sed con regex para transformaciones sofisticadas:"

```bash
# Extraer partes especÃ­ficas
sed 's/.*\([0-9]\{2\}:[0-9]\{2\}:[0-9]\{2\}\).*/\1/' datos/servidor.log

# Formatear datos CSV
echo "Juan,PÃ©rez,Desarrollador" | sed 's/\([^,]*\),\([^,]*\),\(.*\)/Nombre: \1, Apellido: \2, Puesto: \3/'

# Limpiar espacios
echo "  texto con espacios  " | sed 's/^[ \t]*//;s/[ \t]*$//'

# Validar y transformar emails
echo "usuario@dominio.com" | sed 's/\([^@]*\)@\(.*\)/Usuario: \1, Dominio: \2/'
```

### ðŸ“Š Pipeline con Sed

**[PANTALLA: IntegraciÃ³n en pipelines]**

> "Sed brilla cuando se integra en pipelines complejos:"

```bash
# Pipeline completo de procesamiento
cat datos/empleados.txt |
  sed 's/,/ | /g' |                         # Reemplazar comas con separadores
  sed 's/Desarrollador/DEV/g' |             # Abreviar puestos
  sed 's/Administrador/ADMIN/g' |
  head -3

# Procesar logs y generar reporte
cat datos/servidor.log |
  sed '/INFO/d' |                           # Quitar mensajes informativos
  sed 's/ERROR/ðŸ”´ ERROR/g' |                # AÃ±adir emoji a errores
  sed 's/WARNING/ðŸŸ¡ WARNING/g'              # AÃ±adir emoji a warnings
```

---

## ðŸ“Š PARTE 5: AWK - PROCESAMIENTO ESTRUCTURADO (10 minutos)

### ðŸŽ¤ TransiciÃ³n

**[PANTALLA: Logo conceptual de awk]**

> "AWK es un lenguaje de programaciÃ³n completo especializado en procesamiento de texto estructurado. Es perfecto para datos en columnas como CSV, logs, y reportes."

### ðŸ—‚ï¸ Conceptos BÃ¡sicos de AWK

**[PANTALLA: Estructura de AWK]**

> "AWK procesa texto lÃ­nea por lÃ­nea, dividiendo cada lÃ­nea en campos:"

```bash
# Imprimir columnas especÃ­ficas
awk -F',' '{print $1}' datos/empleados.txt        # Primera columna (nombres)
awk -F',' '{print $1, $3}' datos/empleados.txt    # Nombre y salario
awk -F',' '{print $NF}' datos/empleados.txt       # Ãšltima columna (emails)

# InformaciÃ³n sobre campos
awk -F',' '{print NF, $0}' datos/empleados.txt    # NÃºmero de campos por lÃ­nea
awk -F',' '{print "Empleado:", $1, "Salario:", $3}' datos/empleados.txt

# Con headers personalizados
awk -F',' 'BEGIN{print "NOMBRE\t\tSALARIO"} {print $1 "\t\t" $3}' datos/empleados.txt
```

### ðŸ”¢ Operaciones y CÃ¡lculos

**[PANTALLA: AWK para cÃ¡lculos]**

> "AWK puede realizar cÃ¡lculos y operaciones sobre los datos:"

```bash
# Sumar salarios
awk -F',' '{sum += $3} END {print "Total salarios:", sum}' datos/empleados.txt

# Calcular promedio
awk -F',' '{sum += $3; count++} END {print "Promedio:", sum/count}' datos/empleados.txt

# Encontrar mÃ¡ximo y mÃ­nimo
awk -F',' 'NR==1{max=$3; min=$3} {if($3>max) max=$3; if($3<min) min=$3} END {print "Max:", max, "Min:", min}' datos/empleados.txt

# Contar por categorÃ­as
awk -F',' '{count[$2]++} END {for (i in count) print i, count[i]}' datos/empleados.txt
```

### ðŸŽ¯ Filtros y Condiciones

**[PANTALLA: LÃ³gica condicional]**

> "AWK permite filtrar datos con condiciones complejas:"

```bash
# Filtrar por salario
awk -F',' '$3 > 2500 {print $1, $3}' datos/empleados.txt

# MÃºltiples condiciones
awk -F',' '$3 > 2400 && $4 ~ /Madrid|Barcelona/ {print $0}' datos/empleados.txt

# Usar patrones
awk -F',' '/Desarrollador|Backend/ {print $1, "trabaja en", $4}' datos/empleados.txt

# Numerar lÃ­neas que cumplen condiciÃ³n
awk -F',' '$3 > 2500 {printf "%d: %s - %s\n", NR, $1, $3}' datos/empleados.txt
```

### ðŸš€ AWK Avanzado

**[PANTALLA: Funciones avanzadas]**

> "AWK tiene funciones integradas muy Ãºtiles:"

```bash
# Funciones de string
awk -F',' '{print toupper($1), length($1)}' datos/empleados.txt
awk -F',' '{print substr($1, 1, 3)}' datos/empleados.txt  # Primeras 3 letras

# Formateo avanzado
awk -F',' '{printf "%-15s %-12s %6d %-10s\n", $1, $2, $3, $4}' datos/empleados.txt

# Procesar logs con awk
awk '/ERROR/ {errors++} /WARNING/ {warnings++} END {print "Errores:", errors, "Warnings:", warnings}' datos/servidor.log

# Generar reporte completo
awk -F',' '
BEGIN {
    print "=== REPORTE DE EMPLEADOS ==="
    print "Nombre\t\tPuesto\t\tSalario\tCiudad"
    print "----------------------------------------"
}
{
    printf "%-15s %-12s %6d\t%s\n", $1, $2, $3, $4
    total += $3
    count++
}
END {
    print "----------------------------------------"
    printf "Total empleados: %d\n", count
    printf "Gasto total: %d\n", total
    printf "Salario promedio: %.2f\n", total/count
}' datos/empleados.txt
```

---

## ðŸŽ¯ PARTE 6: PROYECTO PRÃCTICO - PROCESADOR DE LOGS (10 minutos)

### ðŸŽ¤ TransiciÃ³n

**[PANTALLA: Logo del proyecto]**

> "Â¡Es hora de integrar todo lo aprendido! Crearemos un procesador de logs completo que combine pipes, grep, sed y awk."

### ðŸ—ï¸ DiseÃ±o del Proyecto

**[PANTALLA: Arquitectura del procesador]**

> "Nuestro procesador analizarÃ¡ logs de servidor y generarÃ¡ reportes detallados:"

```bash
# Crear datos de ejemplo mÃ¡s extensos
cat > datos/servidor_completo.log << 'EOF'
2024-01-15 08:30:01 INFO [web] Usuario juan@empresa.com conectado desde 192.168.1.50
2024-01-15 08:31:15 ERROR [db] Fallo de conexiÃ³n en 192.168.1.100 - timeout 30s
2024-01-15 08:32:30 INFO [web] Usuario maria@empresa.com conectado desde 192.168.1.51
2024-01-15 08:33:45 WARNING [system] Memoria al 85% en servidor web - 6.8GB/8GB
2024-01-15 08:35:00 ERROR [db] Base de datos no responde - conexiÃ³n perdida
2024-01-15 08:36:12 INFO [web] Usuario pedro@empresa.com desconectado
2024-01-15 08:37:30 INFO [backup] Backup completado exitosamente - 2.3GB
2024-01-15 08:38:45 ERROR [api] Timeout en API externa servicio-pagos - 45s
2024-01-15 08:40:00 INFO [web] Usuario ana@empresa.com conectado desde 192.168.1.52
2024-01-15 08:41:30 WARNING [system] CPU al 92% - proceso heavy-task consumiendo recursos
2024-01-15 08:42:15 ERROR [web] Error 500 en /api/users - excepciÃ³n no manejada
2024-01-15 08:43:00 INFO [web] Usuario carlos@empresa.com desconectado
EOF
```

### ðŸ”§ ConstrucciÃ³n del Procesador

**[PANTALLA: Desarrollo paso a paso]**

> "Construyamos el procesador paso a paso, combinando todas las herramientas:"

```bash
# Crear el script procesador
cat > proyectos/procesador_logs.sh << 'EOF'
#!/bin/bash
# Procesador de Logs Avanzado - MÃ³dulo 2
# Combina pipes, grep, sed y awk para anÃ¡lisis completo

set -euo pipefail

LOG_FILE="${1:-datos/servidor_completo.log}"
REPORTE_DIR="reportes"

# Crear directorio de reportes
mkdir -p "$REPORTE_DIR"

echo "ðŸ” PROCESADOR DE LOGS AVANZADO"
echo "=============================="
echo "ðŸ“ Procesando: $LOG_FILE"
echo

# 1. EstadÃ­sticas generales
echo "ðŸ“Š ESTADÃSTICAS GENERALES"
echo "------------------------"
total_lineas=$(wc -l < "$LOG_FILE")
errores=$(grep -c "ERROR" "$LOG_FILE" || echo "0")
warnings=$(grep -c "WARNING" "$LOG_FILE" || echo "0")
info=$(grep -c "INFO" "$LOG_FILE" || echo "0")

echo "Total de entradas: $total_lineas"
echo "Errores: $errores"
echo "Warnings: $warnings"
echo "Info: $info"
echo

# 2. Top errores por componente
echo "ðŸ”´ TOP ERRORES POR COMPONENTE"
echo "----------------------------"
grep "ERROR" "$LOG_FILE" |
sed 's/.*\[\([^]]*\)\].*/\1/' |
sort | uniq -c | sort -nr |
awk '{printf "%-10s: %d errores\n", $2, $1}'
echo

# 3. AnÃ¡lisis de usuarios
echo "ðŸ‘¥ ANÃLISIS DE USUARIOS"
echo "---------------------"
grep -o "[a-zA-Z]\+@[a-zA-Z]\+\.[a-zA-Z]\+" "$LOG_FILE" |
sort | uniq -c | sort -nr |
awk '{printf "%-20s: %d actividades\n", $2, $1}'
echo

# 4. IPs mÃ¡s activas
echo "ðŸŒ IPs MÃS ACTIVAS"
echo "-----------------"
grep -o "192\.168\.[0-9]\+\.[0-9]\+" "$LOG_FILE" |
sort | uniq -c | sort -nr |
awk '{printf "%-15s: %d conexiones\n", $2, $1}'
echo

# 5. Timeline de errores crÃ­ticos
echo "â° TIMELINE DE ERRORES CRÃTICOS"
echo "------------------------------"
grep "ERROR" "$LOG_FILE" |
awk '{
    time = $2
    component = gensub(/.*\[([^]]*)\].*/, "\\1", "g", $0)
    message = substr($0, index($0, "] ") + 2)
    printf "%s [%s] %s\n", time, component, message
}' | head -5
echo

# 6. Generar reporte en archivo
echo "ðŸ“„ Generando reportes en archivos..."

# Reporte de errores
grep "ERROR" "$LOG_FILE" > "$REPORTE_DIR/errores.log"

# Reporte estadÃ­stico
cat > "$REPORTE_DIR/estadisticas.txt" << EOL
REPORTE ESTADÃSTICO - $(date)
===============================

Total de entradas: $total_lineas
Errores: $errores
Warnings: $warnings
Mensajes informativos: $info

Porcentaje de errores: $(echo "scale=2; $errores * 100 / $total_lineas" | bc)%
Porcentaje de warnings: $(echo "scale=2; $warnings * 100 / $total_lineas" | bc)%
EOL

# CSV para anÃ¡lisis posterior
echo "fecha,hora,nivel,componente,mensaje" > "$REPORTE_DIR/logs_procesados.csv"
awk '{
    date = $1
    time = $2
    level = $3
    component = gensub(/.*\[([^]]*)\].*/, "\\1", "g", $0)
    message = gensub(/.*\] (.*)/, "\\1", "g", $0)
    gsub(/,/, ";", message)  # Escapar comas en mensaje
    printf "%s,%s,%s,%s,%s\n", date, time, level, component, message
}' "$LOG_FILE" >> "$REPORTE_DIR/logs_procesados.csv"

echo "âœ… Reportes generados en: $REPORTE_DIR/"
echo "   - errores.log: Solo mensajes de error"
echo "   - estadisticas.txt: Resumen estadÃ­stico"
echo "   - logs_procesados.csv: Datos estructurados para anÃ¡lisis"
echo
echo "ðŸŽ‰ Â¡Procesamiento completado!"
EOF

chmod +x proyectos/procesador_logs.sh
```

### ðŸš€ DemostraciÃ³n del Procesador

**[PANTALLA: EjecuciÃ³n en vivo]**

> "Â¡Ejecutemos nuestro procesador y veamos la magia en acciÃ³n!"

```bash
# Ejecutar el procesador
./proyectos/procesador_logs.sh

# Verificar los reportes generados
ls -la reportes/
head reportes/estadisticas.txt
head reportes/logs_procesados.csv
```

### ðŸŽ“ Extensiones del Proyecto

**[PANTALLA: Ideas para mejoras]**

> "Ideas para expandir el procesador:"

```bash
# VersiÃ³n con parÃ¡metros
echo "Mejoras posibles:"
echo "- Filtros por fecha/hora"
echo "- Alertas automÃ¡ticas por umbral de errores"
echo "- GeneraciÃ³n de grÃ¡ficos con gnuplot"
echo "- EnvÃ­o de reportes por email"
echo "- AnÃ¡lisis de patrones de comportamiento"
echo "- IntegraciÃ³n con herramientas de monitoreo"
```

### ðŸ† Resumen de Logros

**[PANTALLA: Checklist del mÃ³dulo]**

> "Â¡Excelente trabajo! Has dominado:"

- âœ… Pipes para encadenar comandos eficientemente
- âœ… RedirecciÃ³n avanzada de datos
- âœ… Grep con expresiones regulares complejas
- âœ… Sed para ediciÃ³n automatizada de texto
- âœ… AWK para procesamiento estructurado
- âœ… Desarrollo de un procesador de logs completo

### ðŸš€ PrÃ³ximo MÃ³dulo

**[PANTALLA: Preview del MÃ³dulo 3]**

> "En el MÃ³dulo 3 - Variables y Control de Flujo aprenderemos:"

- Variables y expansiÃ³n de parÃ¡metros
- Estructuras condicionales (if, case)
- Bucles (for, while, until)
- Argumentos de script y parsing de opciones
- Proyecto: Sistema de MenÃºs Interactivo

### ðŸŽ¬ Despedida

**[PANTALLA: Logo del bootcamp]**

> "Â¡IncreÃ­ble progreso! Has dado un salto enorme en tus habilidades de Bash. El procesamiento de texto que acabas de dominar es la base de muchÃ­simas tareas de automatizaciÃ³n y administraciÃ³n de sistemas."

> "Practica combinando estos comandos en diferentes escenarios, y nos vemos en el MÃ³dulo 3 donde aÃ±adiremos lÃ³gica e interactividad a nuestros scripts. Â¡Hasta la prÃ³xima!"

---

## ðŸ“‹ CHECKLIST DE PRODUCCIÃ“N

### PreparaciÃ³n de Datos

- [ ] Archivos de ejemplo creados y verificados
- [ ] Scripts de demostraciÃ³n probados
- [ ] Directorios de trabajo organizados
- [ ] Datos realistas para ejercicios

### Durante la GrabaciÃ³n

- [ ] Terminal con fuente grande y legible
- [ ] EjecuciÃ³n paso a paso de cada comando
- [ ] ExplicaciÃ³n del output de cada pipeline
- [ ] Manejo de errores comunes
- [ ] Enfoque en casos de uso reales

### Post-ProducciÃ³n

- [ ] Timestamps para cada secciÃ³n
- [ ] Highlights en comandos importantes
- [ ] Zoom en outputs complejos
- [ ] Overlay con pipeline actual
- [ ] Thumbnail con herramientas principales

---

## ðŸŽ¥ NOTAS TÃ‰CNICAS

### ConfiguraciÃ³n Visual

- **Terminal**: Esquema de colores que diferencie grep matches
- **Editor**: Syntax highlighting para scripts
- **Split screen**: Terminal y editor simultÃ¡neamente
- **Zoom**: Nivel apropiado para comandos largos

### Puntos Clave

- Explicar cada parte de pipelines complejos
- Mostrar alternatives para diferentes casos
- Enfatizar eficiencia vs legibilidad
- Demostrar debugging de pipelines
- Incluir mejores prÃ¡cticas de rendimiento

---

**Â¡Listo para crear un video excepcional del MÃ³dulo 2! ðŸš€**
