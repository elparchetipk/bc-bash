# Bootcamp Bash - M√≥dulo 2: Desarrollo Paso a Paso

## Gui√≥n para Video de YouTube - Comandos Avanzados y Pipes

---

## üìã INFORMACI√ìN DEL VIDEO

**T√≠tulo:** "Bootcamp Bash - M√≥dulo 2: Comandos Avanzados y Pipes - Dominando el Procesamiento de Texto"

**Duraci√≥n Estimada:** 50-65 minutos

**Audiencia:** Estudiantes que completaron el M√≥dulo 1 y quieren dominar el procesamiento de texto en Bash

**Objetivo:** Dominar pipes, redirecci√≥n y herramientas de procesamiento de texto para crear flujos de trabajo potentes

---

## üéØ OBJETIVOS DE APRENDIZAJE

Al finalizar este video, los estudiantes podr√°n:

1. **Dominar** el uso de pipes para encadenar comandos
2. **Implementar** redirecci√≥n avanzada de entrada y salida
3. **Procesar** texto eficientemente con grep, sed, awk
4. **Utilizar** wildcards y pattern matching avanzado
5. **Crear** filtros complejos de datos
6. **Desarrollar** un procesador de logs funcional

---

## üìù ESTRUCTURA DEL VIDEO

### PARTE 1: INTRODUCCI√ìN Y REPASO (5 minutos)

### PARTE 2: PIPES Y REDIRECCI√ìN AVANZADA (15 minutos)

### PARTE 3: MAESTR√çA EN GREP Y EXPRESIONES REGULARES (15 minutos)

### PARTE 4: SED - EL EDITOR DE FLUJO (10 minutos)

### PARTE 5: AWK - PROCESAMIENTO ESTRUCTURADO (10 minutos)

### PARTE 6: PROYECTO PR√ÅCTICO - PROCESADOR DE LOGS (10 minutos)

---

## üé¨ PARTE 1: INTRODUCCI√ìN Y REPASO (5 minutos)

### üé§ Gui√≥n de Presentaci√≥n

**[PANTALLA: Logo del M√≥dulo 2 con SVG]**

> "¬°Bienvenidos de vuelta al Bootcamp Bash! Soy tu instructor y hoy vamos a elevar tus habilidades al siguiente nivel con el M√≥dulo 2: Comandos Avanzados y Pipes."

**[TRANSICI√ìN: Mostrar logros del M√≥dulo 1]**

> "En el m√≥dulo anterior aprendimos los fundamentos: navegaci√≥n, comandos b√°sicos, nuestro primer script. Hoy vamos a aprender a combinar estos comandos para crear flujos de trabajo realmente potentes."

### üìä El Poder de los Pipes

**[PANTALLA: Diagrama conceptual de pipes]**

> "Los pipes son como tuber√≠as que conectan comandos. La salida de un comando se convierte en la entrada del siguiente, creando cadenas de procesamiento incre√≠blemente eficientes."

**[PANTALLA: Ejemplo visual]**

> "Imagina procesar un archivo de 10GB de logs en segundos, extraer informaci√≥n espec√≠fica, formatearla y generar un reporte. Eso es el poder que vamos a dominar hoy."

### üó∫Ô∏è Roadmap del M√≥dulo 2

**[PANTALLA: Estructura del m√≥dulo]**

> "En este m√≥dulo cubriremos:
>
> - Pipes y redirecci√≥n avanzada
> - Grep con expresiones regulares
> - Sed para edici√≥n de flujo
> - Awk para procesamiento estructurado
> - Wildcards y pattern matching
> - Proyecto: Procesador de Logs Avanzado"

### üîÑ Repaso R√°pido

**[PANTALLA: Terminal]**

> "Repasemos r√°pidamente el M√≥dulo 1 creando algunos archivos de prueba:"

```bash
# Configurar entorno para el m√≥dulo 2
cd ~/bootcamp-bash
mkdir -p modulo2/{ejercicios,proyectos,datos}
cd modulo2

# Crear datos de ejemplo
cat > datos/empleados.txt << 'EOF'
Juan P√©rez,Desarrollador,2500,Madrid,juan@empresa.com
Mar√≠a Garc√≠a,Dise√±adora,2200,Barcelona,maria@empresa.com
Pedro L√≥pez,Administrador,2800,Valencia,pedro@empresa.com
Ana Mart√≠n,DevOps,3000,Sevilla,ana@empresa.com
Carlos Ruiz,Backend,2600,Bilbao,carlos@empresa.com
Laura Torres,Frontend,2400,M√°laga,laura@empresa.com
EOF

echo "‚úÖ Datos preparados para el m√≥dulo 2"
```

---

## üîó PARTE 2: PIPES Y REDIRECCI√ìN AVANZADA (15 minutos)

### üé§ Transici√≥n

**[PANTALLA: Diagrama de flujo de datos]**

> "Los pipes son la columna vertebral del procesamiento en Bash. Permiten que la salida de un comando se convierta autom√°ticamente en la entrada del siguiente."

### üö∞ Fundamentos de Pipes

**[PANTALLA: Demostraci√≥n en vivo]**

> "Empezamos con pipes b√°sicos y vamos escalando la complejidad:"

```bash
# Pipe b√°sico
cat datos/empleados.txt | head -3

# Encadenar m√∫ltiples comandos
cat datos/empleados.txt | grep "Madrid" | cut -d',' -f1

# Contar elementos
cat datos/empleados.txt | wc -l
ls | wc -l

# Ordenar y mostrar √∫nicos
echo -e "manzana\nbanana\nmanzana\nnaranja\nbanana" | sort | uniq

# Pipeline complejo
cat datos/empleados.txt | grep -v "Dise√±adora" | sort | head -2
```

### üì§ Redirecci√≥n Avanzada

**[PANTALLA: Explicaci√≥n de redirecci√≥n]**

> "La redirecci√≥n nos permite controlar exactamente d√≥nde van los datos:"

```bash
# Redirecci√≥n b√°sica
echo "Log de inicio: $(date)" > logs/sistema.log
echo "Proceso completado" >> logs/sistema.log

# Redirecci√≥n de errores
ls archivo_inexistente 2> errores.log
ls archivo_inexistente 2>> errores.log

# Redireccionar salida y errores por separado
comando_ejemplo > salida.txt 2> errores.txt

# Redireccionar ambos al mismo archivo
ls /etc /directorio_falso > completo.log 2>&1

# Here documents
cat << 'EOF' > script_generado.sh
#!/bin/bash
echo "Script generado autom√°ticamente"
date
EOF
```

### üîÑ Pipes Avanzados y Tee

**[PANTALLA: Demostraci√≥n de tee]**

> "El comando tee es como una 'T' en fontaner√≠a - divide el flujo:"

```bash
# Guardar y mostrar simult√°neamente
cat datos/empleados.txt | grep "Madrid" | tee madrid_empleados.txt

# M√∫ltiples destinos
echo "Mensaje importante" | tee archivo1.txt archivo2.txt archivo3.txt

# A√±adir a archivos existentes
date | tee -a logs/acceso.log logs/general.log

# Pipeline con tee para debugging
cat datos/empleados.txt |
  grep "Desarrollador\|Backend" |
  tee desarrolladores_temp.txt |
  cut -d',' -f1,3 |
  sort -t',' -k2 -n
```

### üîß Herramientas de Filtrado

**[PANTALLA: Comandos de filtrado]**

> "Herramientas espec√≠ficas para filtrar y transformar datos:"

```bash
# cut - extraer columnas
cut -d',' -f1,3 datos/empleados.txt  # Nombre y salario
cut -d',' -f1-2 datos/empleados.txt  # Primeras dos columnas

# sort - ordenamiento avanzado
sort datos/empleados.txt                    # Orden alfab√©tico
sort -t',' -k3 -n datos/empleados.txt      # Por salario (num√©rico)
sort -t',' -k4 datos/empleados.txt         # Por ciudad

# uniq - elementos √∫nicos
cut -d',' -f4 datos/empleados.txt | sort | uniq  # Ciudades √∫nicas
cut -d',' -f4 datos/empleados.txt | sort | uniq -c  # Con conteos

# tr - transformar caracteres
cat datos/empleados.txt | tr ',' '\t'       # Comas a tabs
echo "TEXTO EN MAY√öSCULAS" | tr 'A-Z' 'a-z'  # A min√∫sculas
```

---

## üîç PARTE 3: MAESTR√çA EN GREP Y EXPRESIONES REGULARES (15 minutos)

### üé§ Transici√≥n

**[PANTALLA: Logo de grep con ejemplos]**

> "Grep es probablemente la herramienta de b√∫squeda m√°s potente en Unix. Vamos a dominar desde b√∫squedas b√°sicas hasta expresiones regulares complejas."

### üéØ Grep B√°sico a Avanzado

**[PANTALLA: Demostraci√≥n progresiva]**

> "Empezamos con grep b√°sico y escalamos la complejidad:"

```bash
# Crear archivo de logs para practicar
cat > datos/servidor.log << 'EOF'
2024-01-15 08:30:01 INFO Usuario juan@empresa.com conectado
2024-01-15 08:31:15 ERROR Fallo de conexi√≥n en 192.168.1.100
2024-01-15 08:32:30 INFO Usuario maria@empresa.com conectado
2024-01-15 08:33:45 WARNING Memoria al 85% en servidor web
2024-01-15 08:35:00 ERROR Base de datos no responde
2024-01-15 08:36:12 INFO Usuario pedro@empresa.com desconectado
2024-01-15 08:37:30 INFO Backup completado exitosamente
2024-01-15 08:38:45 ERROR Timeout en API externa
EOF

# B√∫squedas b√°sicas
grep "ERROR" datos/servidor.log
grep -i "info" datos/servidor.log           # Case insensitive
grep -n "WARNING" datos/servidor.log        # Con n√∫meros de l√≠nea
grep -v "INFO" datos/servidor.log           # Excluir l√≠neas
grep -c "ERROR" datos/servidor.log          # Contar coincidencias
```

### üé≠ Expresiones Regulares

**[PANTALLA: Introducci√≥n a regex]**

> "Las expresiones regulares son patrones que describen texto. Son incre√≠blemente poderosas:"

```bash
# Metacaracteres b√°sicos
grep "^2024" datos/servidor.log             # L√≠neas que empiezan con 2024
grep "conectado$" datos/servidor.log        # L√≠neas que terminan con 'conectado'
grep "08:3[0-9]" datos/servidor.log         # Minutos entre 30-39

# Clases de caracteres
grep "[0-9]" datos/servidor.log             # Cualquier d√≠gito
grep "[A-Z][A-Z][A-Z]" datos/servidor.log   # Tres may√∫sculas seguidas
grep "[0-9]\{1,3\}\.[0-9]\{1,3\}" datos/servidor.log  # IPs parciales

# Cuantificadores
grep "o\+" datos/servidor.log               # Una o m√°s 'o'
grep "o*" datos/servidor.log                # Cero o m√°s 'o'
grep "o\?" datos/servidor.log               # Cero o una 'o'

# Ejemplos pr√°cticos
grep "[a-zA-Z]\+@[a-zA-Z]\+\.[a-zA-Z]\+" datos/servidor.log  # Emails
grep "192\.168\.[0-9]\+\.[0-9]\+" datos/servidor.log        # IPs locales
```

### üîß Grep con Opciones Avanzadas

**[PANTALLA: Opciones potentes]**

> "Opciones que multiplican el poder de grep:"

```bash
# B√∫squeda en m√∫ltiples archivos
grep -r "ERROR" logs/                       # Recursivo en directorio
grep -l "ERROR" *.log                       # Solo nombres de archivos
grep -L "SUCCESS" *.log                     # Archivos SIN el patr√≥n

# Contexto alrededor de coincidencias
grep -A 2 "ERROR" datos/servidor.log        # 2 l√≠neas despu√©s
grep -B 2 "ERROR" datos/servidor.log        # 2 l√≠neas antes
grep -C 2 "ERROR" datos/servidor.log        # 2 l√≠neas antes y despu√©s

# M√∫ltiples patrones
grep -E "ERROR|WARNING" datos/servidor.log   # OR l√≥gico
grep -F "192.168.1.100" datos/servidor.log   # Literal (sin regex)

# Pipeline con grep
cat datos/empleados.txt | grep "Madrid\|Barcelona" | grep "Desarrollador"
```

### üöÄ Casos de Uso Reales

**[PANTALLA: Ejemplos pr√°cticos]**

> "Casos reales donde grep es imprescindible:"

```bash
# Analizar logs de Apache
grep "404" /var/log/apache2/access.log | head -10

# Encontrar procesos
ps aux | grep python | grep -v grep

# Buscar configuraciones
grep -r "port.*80" /etc/ 2>/dev/null

# Extraer IPs √∫nicas de logs
grep -o "[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}\.[0-9]\{1,3\}" datos/servidor.log | sort | uniq
```

---

## ‚úèÔ∏è PARTE 4: SED - EL EDITOR DE FLUJO (10 minutos)

### üé§ Transici√≥n

**[PANTALLA: Logo conceptual de sed]**

> "Sed significa 'Stream Editor' - editor de flujo. Es como un editor de texto que trabaja con pipes, perfecto para transformaciones automatizadas."

### üîÑ Operaciones B√°sicas de Sed

**[PANTALLA: Demostraci√≥n en vivo]**

> "Sed trabaja l√≠nea por l√≠nea, aplicando transformaciones seg√∫n patrones:"

```bash
# Sustituir texto (primera ocurrencia por l√≠nea)
sed 's/ERROR/FALLO/' datos/servidor.log

# Sustituir todas las ocurrencias
sed 's/ERROR/FALLO/g' datos/servidor.log

# Sustituir solo en l√≠neas espec√≠ficas
sed '2s/INFO/INFORMACI√ìN/' datos/servidor.log

# Guardar cambios en archivo
sed 's/ERROR/FALLO/g' datos/servidor.log > datos/servidor_modificado.log

# Modificar archivo in-place (¬°CUIDADO!)
cp datos/servidor.log datos/servidor_backup.log
sed -i 's/WARNING/ALERTA/g' datos/servidor_backup.log
```

### üéØ Operaciones Avanzadas

**[PANTALLA: Comandos complejos]**

> "Sed puede hacer mucho m√°s que sustituir texto:"

```bash
# Eliminar l√≠neas
sed '/INFO/d' datos/servidor.log            # Eliminar l√≠neas con INFO
sed '1d' datos/servidor.log                 # Eliminar primera l√≠nea
sed '$d' datos/servidor.log                 # Eliminar √∫ltima l√≠nea
sed '2,4d' datos/servidor.log               # Eliminar l√≠neas 2-4

# Agregar l√≠neas
sed '1i\=== INICIO DE LOG ===' datos/servidor.log    # Insertar al principio
sed '$a\=== FIN DE LOG ===' datos/servidor.log       # Agregar al final

# Imprimir l√≠neas espec√≠ficas
sed -n '2,4p' datos/servidor.log            # Solo l√≠neas 2-4
sed -n '/ERROR/p' datos/servidor.log        # Solo l√≠neas con ERROR

# M√∫ltiples operaciones
sed -e 's/ERROR/FALLO/g' -e 's/WARNING/ALERTA/g' datos/servidor.log
```

### üîß Sed con Expresiones Regulares

**[PANTALLA: Regex en sed]**

> "Combinando sed con regex para transformaciones sofisticadas:"

```bash
# Extraer partes espec√≠ficas
sed 's/.*\([0-9]\{2\}:[0-9]\{2\}:[0-9]\{2\}\).*/\1/' datos/servidor.log

# Formatear datos CSV
echo "Juan,P√©rez,Desarrollador" | sed 's/\([^,]*\),\([^,]*\),\(.*\)/Nombre: \1, Apellido: \2, Puesto: \3/'

# Limpiar espacios
echo "  texto con espacios  " | sed 's/^[ \t]*//;s/[ \t]*$//'

# Validar y transformar emails
echo "usuario@dominio.com" | sed 's/\([^@]*\)@\(.*\)/Usuario: \1, Dominio: \2/'
```

### üìä Pipeline con Sed

**[PANTALLA: Integraci√≥n en pipelines]**

> "Sed brilla cuando se integra en pipelines complejos:"

```bash
# Pipeline completo de procesamiento
cat datos/empleados.txt |
  sed 's/,/ | /g' |                         # Reemplazar comas con separadores
  sed 's/Desarrollador/DEV/g' |             # Abreviar puestos
  sed 's/Administrador/ADMIN/g' |
  head -3

# Procesar logs y generar reporte
cat datos/servidor.log |
  sed '/INFO/d' |                           # Quitar mensajes informativos
  sed 's/ERROR/üî¥ ERROR/g' |                # A√±adir emoji a errores
  sed 's/WARNING/üü° WARNING/g'              # A√±adir emoji a warnings
```

---

## üìä PARTE 5: AWK - PROCESAMIENTO ESTRUCTURADO (10 minutos)

### üé§ Transici√≥n

**[PANTALLA: Logo conceptual de awk]**

> "AWK es un lenguaje de programaci√≥n completo especializado en procesamiento de texto estructurado. Es perfecto para datos en columnas como CSV, logs, y reportes."

### üóÇÔ∏è Conceptos B√°sicos de AWK

**[PANTALLA: Estructura de AWK]**

> "AWK procesa texto l√≠nea por l√≠nea, dividiendo cada l√≠nea en campos:"

```bash
# Imprimir columnas espec√≠ficas
awk -F',' '{print $1}' datos/empleados.txt        # Primera columna (nombres)
awk -F',' '{print $1, $3}' datos/empleados.txt    # Nombre y salario
awk -F',' '{print $NF}' datos/empleados.txt       # √öltima columna (emails)

# Informaci√≥n sobre campos
awk -F',' '{print NF, $0}' datos/empleados.txt    # N√∫mero de campos por l√≠nea
awk -F',' '{print "Empleado:", $1, "Salario:", $3}' datos/empleados.txt

# Con headers personalizados
awk -F',' 'BEGIN{print "NOMBRE\t\tSALARIO"} {print $1 "\t\t" $3}' datos/empleados.txt
```

### üî¢ Operaciones y C√°lculos

**[PANTALLA: AWK para c√°lculos]**

> "AWK puede realizar c√°lculos y operaciones sobre los datos:"

```bash
# Sumar salarios
awk -F',' '{sum += $3} END {print "Total salarios:", sum}' datos/empleados.txt

# Calcular promedio
awk -F',' '{sum += $3; count++} END {print "Promedio:", sum/count}' datos/empleados.txt

# Encontrar m√°ximo y m√≠nimo
awk -F',' 'NR==1{max=$3; min=$3} {if($3>max) max=$3; if($3<min) min=$3} END {print "Max:", max, "Min:", min}' datos/empleados.txt

# Contar por categor√≠as
awk -F',' '{count[$2]++} END {for (i in count) print i, count[i]}' datos/empleados.txt
```

### üéØ Filtros y Condiciones

**[PANTALLA: L√≥gica condicional]**

> "AWK permite filtrar datos con condiciones complejas:"

```bash
# Filtrar por salario
awk -F',' '$3 > 2500 {print $1, $3}' datos/empleados.txt

# M√∫ltiples condiciones
awk -F',' '$3 > 2400 && $4 ~ /Madrid|Barcelona/ {print $0}' datos/empleados.txt

# Usar patrones
awk -F',' '/Desarrollador|Backend/ {print $1, "trabaja en", $4}' datos/empleados.txt

# Numerar l√≠neas que cumplen condici√≥n
awk -F',' '$3 > 2500 {printf "%d: %s - %s\n", NR, $1, $3}' datos/empleados.txt
```

### üöÄ AWK Avanzado

**[PANTALLA: Funciones avanzadas]**

> "AWK tiene funciones integradas muy √∫tiles:"

```bash
# Funciones de string
awk -F',' '{print toupper($1), length($1)}' datos/empleados.txt
awk -F',' '{print substr($1, 1, 3)}' datos/empleados.txt  # Primeras 3 letras

# Formateo avanzado
awk -F',' '{printf "%-15s %-12s %6d %-10s\n", $1, $2, $3, $4}' datos/empleados.txt

# Procesar logs con awk
awk '/ERROR/ {errors++} /WARNING/ {warnings++} END {print "Errores:", errors, "Warnings:", warnings}' datos/servidor.log

# Generar reporte completo
awk -F',' '
BEGIN {
    print "=== REPORTE DE EMPLEADOS ==="
    print "Nombre\t\tPuesto\t\tSalario\tCiudad"
    print "----------------------------------------"
}
{
    printf "%-15s %-12s %6d\t%s\n", $1, $2, $3, $4
    total += $3
    count++
}
END {
    print "----------------------------------------"
    printf "Total empleados: %d\n", count
    printf "Gasto total: %d\n", total
    printf "Salario promedio: %.2f\n", total/count
}' datos/empleados.txt
```

---

## üéØ PARTE 6: PROYECTO PR√ÅCTICO - PROCESADOR DE LOGS (10 minutos)

### üé§ Transici√≥n

**[PANTALLA: Logo del proyecto]**

> "¬°Es hora de integrar todo lo aprendido! Crearemos un procesador de logs completo que combine pipes, grep, sed y awk."

### üèóÔ∏è Dise√±o del Proyecto

**[PANTALLA: Arquitectura del procesador]**

> "Nuestro procesador analizar√° logs de servidor y generar√° reportes detallados:"

```bash
# Crear datos de ejemplo m√°s extensos
cat > datos/servidor_completo.log << 'EOF'
2024-01-15 08:30:01 INFO [web] Usuario juan@empresa.com conectado desde 192.168.1.50
2024-01-15 08:31:15 ERROR [db] Fallo de conexi√≥n en 192.168.1.100 - timeout 30s
2024-01-15 08:32:30 INFO [web] Usuario maria@empresa.com conectado desde 192.168.1.51
2024-01-15 08:33:45 WARNING [system] Memoria al 85% en servidor web - 6.8GB/8GB
2024-01-15 08:35:00 ERROR [db] Base de datos no responde - conexi√≥n perdida
2024-01-15 08:36:12 INFO [web] Usuario pedro@empresa.com desconectado
2024-01-15 08:37:30 INFO [backup] Backup completado exitosamente - 2.3GB
2024-01-15 08:38:45 ERROR [api] Timeout en API externa servicio-pagos - 45s
2024-01-15 08:40:00 INFO [web] Usuario ana@empresa.com conectado desde 192.168.1.52
2024-01-15 08:41:30 WARNING [system] CPU al 92% - proceso heavy-task consumiendo recursos
2024-01-15 08:42:15 ERROR [web] Error 500 en /api/users - excepci√≥n no manejada
2024-01-15 08:43:00 INFO [web] Usuario carlos@empresa.com desconectado
EOF
```

### üîß Construcci√≥n del Procesador

**[PANTALLA: Desarrollo paso a paso]**

> "Construyamos el procesador paso a paso, combinando todas las herramientas:"

```bash
# Crear el script procesador
cat > proyectos/procesador_logs.sh << 'EOF'
#!/bin/bash
# Procesador de Logs Avanzado - M√≥dulo 2
# Combina pipes, grep, sed y awk para an√°lisis completo

set -euo pipefail

LOG_FILE="${1:-datos/servidor_completo.log}"
REPORTE_DIR="reportes"

# Crear directorio de reportes
mkdir -p "$REPORTE_DIR"

echo "üîç PROCESADOR DE LOGS AVANZADO"
echo "=============================="
echo "üìÅ Procesando: $LOG_FILE"
echo

# 1. Estad√≠sticas generales
echo "üìä ESTAD√çSTICAS GENERALES"
echo "------------------------"
total_lineas=$(wc -l < "$LOG_FILE")
errores=$(grep -c "ERROR" "$LOG_FILE" || echo "0")
warnings=$(grep -c "WARNING" "$LOG_FILE" || echo "0")
info=$(grep -c "INFO" "$LOG_FILE" || echo "0")

echo "Total de entradas: $total_lineas"
echo "Errores: $errores"
echo "Warnings: $warnings"
echo "Info: $info"
echo

# 2. Top errores por componente
echo "üî¥ TOP ERRORES POR COMPONENTE"
echo "----------------------------"
grep "ERROR" "$LOG_FILE" |
sed 's/.*\[\([^]]*\)\].*/\1/' |
sort | uniq -c | sort -nr |
awk '{printf "%-10s: %d errores\n", $2, $1}'
echo

# 3. An√°lisis de usuarios
echo "üë• AN√ÅLISIS DE USUARIOS"
echo "---------------------"
grep -o "[a-zA-Z]\+@[a-zA-Z]\+\.[a-zA-Z]\+" "$LOG_FILE" |
sort | uniq -c | sort -nr |
awk '{printf "%-20s: %d actividades\n", $2, $1}'
echo

# 4. IPs m√°s activas
echo "üåê IPs M√ÅS ACTIVAS"
echo "-----------------"
grep -o "192\.168\.[0-9]\+\.[0-9]\+" "$LOG_FILE" |
sort | uniq -c | sort -nr |
awk '{printf "%-15s: %d conexiones\n", $2, $1}'
echo

# 5. Timeline de errores cr√≠ticos
echo "‚è∞ TIMELINE DE ERRORES CR√çTICOS"
echo "------------------------------"
grep "ERROR" "$LOG_FILE" |
awk '{
    time = $2
    component = gensub(/.*\[([^]]*)\].*/, "\\1", "g", $0)
    message = substr($0, index($0, "] ") + 2)
    printf "%s [%s] %s\n", time, component, message
}' | head -5
echo

# 6. Generar reporte en archivo
echo "üìÑ Generando reportes en archivos..."

# Reporte de errores
grep "ERROR" "$LOG_FILE" > "$REPORTE_DIR/errores.log"

# Reporte estad√≠stico
cat > "$REPORTE_DIR/estadisticas.txt" << EOL
REPORTE ESTAD√çSTICO - $(date)
===============================

Total de entradas: $total_lineas
Errores: $errores
Warnings: $warnings
Mensajes informativos: $info

Porcentaje de errores: $(echo "scale=2; $errores * 100 / $total_lineas" | bc)%
Porcentaje de warnings: $(echo "scale=2; $warnings * 100 / $total_lineas" | bc)%
EOL

# CSV para an√°lisis posterior
echo "fecha,hora,nivel,componente,mensaje" > "$REPORTE_DIR/logs_procesados.csv"
awk '{
    date = $1
    time = $2
    level = $3
    component = gensub(/.*\[([^]]*)\].*/, "\\1", "g", $0)
    message = gensub(/.*\] (.*)/, "\\1", "g", $0)
    gsub(/,/, ";", message)  # Escapar comas en mensaje
    printf "%s,%s,%s,%s,%s\n", date, time, level, component, message
}' "$LOG_FILE" >> "$REPORTE_DIR/logs_procesados.csv"

echo "‚úÖ Reportes generados en: $REPORTE_DIR/"
echo "   - errores.log: Solo mensajes de error"
echo "   - estadisticas.txt: Resumen estad√≠stico"
echo "   - logs_procesados.csv: Datos estructurados para an√°lisis"
echo
echo "üéâ ¬°Procesamiento completado!"
EOF

chmod +x proyectos/procesador_logs.sh
```

### üöÄ Demostraci√≥n del Procesador

**[PANTALLA: Ejecuci√≥n en vivo]**

> "¬°Ejecutemos nuestro procesador y veamos la magia en acci√≥n!"

```bash
# Ejecutar el procesador
./proyectos/procesador_logs.sh

# Verificar los reportes generados
ls -la reportes/
head reportes/estadisticas.txt
head reportes/logs_procesados.csv
```

### üéì Extensiones del Proyecto

**[PANTALLA: Ideas para mejoras]**

> "Ideas para expandir el procesador:"

```bash
# Versi√≥n con par√°metros
echo "Mejoras posibles:"
echo "- Filtros por fecha/hora"
echo "- Alertas autom√°ticas por umbral de errores"
echo "- Generaci√≥n de gr√°ficos con gnuplot"
echo "- Env√≠o de reportes por email"
echo "- An√°lisis de patrones de comportamiento"
echo "- Integraci√≥n con herramientas de monitoreo"
```

### üèÜ Resumen de Logros

**[PANTALLA: Checklist del m√≥dulo]**

> "¬°Excelente trabajo! Has dominado:"

- ‚úÖ Pipes para encadenar comandos eficientemente
- ‚úÖ Redirecci√≥n avanzada de datos
- ‚úÖ Grep con expresiones regulares complejas
- ‚úÖ Sed para edici√≥n automatizada de texto
- ‚úÖ AWK para procesamiento estructurado
- ‚úÖ Desarrollo de un procesador de logs completo

### üöÄ Pr√≥ximo M√≥dulo

**[PANTALLA: Preview del M√≥dulo 3]**

> "En el M√≥dulo 3 - Variables y Control de Flujo aprenderemos:"

- Variables y expansi√≥n de par√°metros
- Estructuras condicionales (if, case)
- Bucles (for, while, until)
- Argumentos de script y parsing de opciones
- Proyecto: Sistema de Men√∫s Interactivo

### üé¨ Despedida

**[PANTALLA: Logo del bootcamp]**

> "¬°Incre√≠ble progreso! Has dado un salto enorme en tus habilidades de Bash. El procesamiento de texto que acabas de dominar es la base de much√≠simas tareas de automatizaci√≥n y administraci√≥n de sistemas."

> "Practica combinando estos comandos en diferentes escenarios, y nos vemos en el M√≥dulo 3 donde a√±adiremos l√≥gica e interactividad a nuestros scripts. ¬°Hasta la pr√≥xima!"

---

## üìã CHECKLIST DE PRODUCCI√ìN

### Preparaci√≥n de Datos

- [ ] Archivos de ejemplo creados y verificados
- [ ] Scripts de demostraci√≥n probados
- [ ] Directorios de trabajo organizados
- [ ] Datos realistas para ejercicios

### Durante la Grabaci√≥n

- [ ] Terminal con fuente grande y legible
- [ ] Ejecuci√≥n paso a paso de cada comando
- [ ] Explicaci√≥n del output de cada pipeline
- [ ] Manejo de errores comunes
- [ ] Enfoque en casos de uso reales

### Post-Producci√≥n

- [ ] Timestamps para cada secci√≥n
- [ ] Highlights en comandos importantes
- [ ] Zoom en outputs complejos
- [ ] Overlay con pipeline actual
- [ ] Thumbnail con herramientas principales

---

## üé• NOTAS T√âCNICAS

### Configuraci√≥n Visual

- **Terminal**: Esquema de colores que diferencie grep matches
- **Editor**: Syntax highlighting para scripts
- **Split screen**: Terminal y editor simult√°neamente
- **Zoom**: Nivel apropiado para comandos largos

### Puntos Clave

- Explicar cada parte de pipelines complejos
- Mostrar alternatives para diferentes casos
- Enfatizar eficiencia vs legibilidad
- Demostrar debugging de pipelines
- Incluir mejores pr√°cticas de rendimiento

---

**¬°Listo para crear un video excepcional del M√≥dulo 2! üöÄ**
