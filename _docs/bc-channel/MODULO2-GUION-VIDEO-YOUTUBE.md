# Bootcamp Bash - M√≥dulo 2: Gui√≥n para Video de YouTube

## üìã INFORMACI√ìN DEL VIDEO

**T√≠tulo:** "Bootcamp Bash - M√≥dulo 2: Comandos Avanzados y Pipes - Dominando el Procesamiento de Texto"

**Duraci√≥n Estimada:** 50-65 minutos

**Audiencia:** Desarrolladores y administradores de sistemas que ya dominan los fundamentos del m√≥dulo 1

**Objetivo:** Dominar el procesamiento de texto avanzado y la combinaci√≥n de comandos mediante pipes para crear flujos de trabajo potentes

---

## üéØ OBJETIVOS DE APRENDIZAJE

Al finalizar este video, los estudiantes podr√°n:

1. **Procesar** archivos de texto con grep, sed, awk y herramientas de filtrado
2. **Combinar** comandos usando pipes para crear flujos de trabajo complejos
3. **Redirigir** entrada, salida y errores de manera eficiente
4. **Aplicar** expresiones regulares b√°sicas para b√∫squedas avanzadas
5. **Crear** scripts que procesen grandes vol√∫menes de datos
6. **Automatizar** tareas de an√°lisis de logs y archivos de texto

---

## üìù ESTRUCTURA DEL VIDEO

### PARTE 1: INTRODUCCI√ìN Y CONTEXTO (8 minutos)

### PARTE 2: PROCESAMIENTO DE TEXTO B√ÅSICO (15 minutos)

### PARTE 3: PIPES Y FLUJOS DE DATOS (12 minutos)

### PARTE 4: REDIRECCI√ìN AVANZADA (8 minutos)

### PARTE 5: EXPRESIONES REGULARES Y AWK (15 minutos)

### PARTE 6: PROYECTO PR√ÅCTICO - ANALIZADOR DE LOGS (12 minutos)

---

## üé¨ PARTE 1: INTRODUCCI√ìN Y CONTEXTO (8 minutos)

### üé§ Gui√≥n de Presentaci√≥n

**[PANTALLA: Logo del M√≥dulo 2 con animaci√≥n de pipes]**

> "¬°Bienvenidos de vuelta al Bootcamp Bash! Soy tu instructor y hoy vamos a explorar uno de los aspectos m√°s poderosos de bash: el procesamiento de texto y los pipes. Si el m√≥dulo 1 te ense√±√≥ a caminar, el m√≥dulo 2 te ense√±ar√° a correr."

**[TRANSICI√ìN: Estad√≠sticas del procesamiento de texto]**

> "¬øSab√≠as que un administrador de sistemas promedio procesa gigabytes de logs diariamente? ¬øO que las empresas como Netflix analizan petabytes de datos usando herramientas que aprender√°s hoy? Los comandos que dominar√°s pueden procesar millones de l√≠neas en segundos."

### üìä El Poder del Procesamiento de Texto

**[PANTALLA: Infograf√≠a con casos de uso reales]**

> "El procesamiento de texto no es solo teor√≠a. Es la diferencia entre:
>
> - Analizar logs manualmente durante horas vs. automatizarlo en minutos
> - Procesar archivos CSV gigantes vs. extraer insights instant√°neamente
> - Depurar problemas reactivamente vs. monitorizarlos proactivamente
> - Ser un usuario de terminal vs. ser un maestro de la automatizaci√≥n"

**[PANTALLA: Demostraci√≥n r√°pida del resultado final]**

> "Al final de este m√≥dulo, habr√°s creado un analizador de logs que puede procesar gigabytes de datos y generar reportes autom√°ticos. Pero m√°s importante, entender√°s la filosof√≠a Unix: herramientas simples que se combinan para resolver problemas complejos."

### üß† La Filosof√≠a Unix en Acci√≥n

**[PANTALLA: Diagrama de la filosof√≠a Unix]**

> "Unix se basa en un principio brillante: 'Haz una cosa y hazla bien'. En lugar de tener un comando monstruoso que haga todo, tenemos herramientas especializadas:
>
> - `grep`: El detective que encuentra patrones
> - `sed`: El editor que transforma texto
> - `awk`: El analista que procesa datos estructurados
> - `sort`: El organizador que ordena informaci√≥n
> - `uniq`: El filtro que elimina duplicados"

### üîó El Poder de la Combinaci√≥n

**[PANTALLA: Animaci√≥n de pipes conectando comandos]**

> "La magia sucede cuando conectamos estas herramientas con pipes. Es como crear una cadena de montaje donde cada comando especializado contribuye al resultado final. Un pipe no es solo un s√≠mbolo `|`, es una filosof√≠a de trabajo."

### üó∫Ô∏è Roadmap del M√≥dulo 2

**[PANTALLA: Cronograma visual interactivo]**

> "Hoy construiremos conocimiento progresivamente:
>
> **Fundamentos:** grep, sort, uniq, wc - Las herramientas b√°sicas
> **Conexiones:** Pipes y redirecci√≥n - Conectando el flujo
> **Transformaci√≥n:** sed y expresiones regulares - Modificando datos
> **An√°lisis:** awk avanzado - Procesamiento inteligente
> **Proyecto:** Analizador de logs completo - Aplicaci√≥n real"

**[TRANSICI√ìN: Preparar terminal]**

> "¬øPreparados para convertirse en maestros del procesamiento de texto? ¬°Abramos la terminal y comencemos!"

---

## üîç PARTE 2: PROCESAMIENTO DE TEXTO B√ÅSICO (15 minutos)

### üé§ Transici√≥n

**[PANTALLA: Icono de lupa con texto]**

> "Comenzamos con las herramientas fundamentales. Imaginen que son detectives digitales y estos comandos son sus herramientas de investigaci√≥n. Cada uno tiene un prop√≥sito espec√≠fico y cuando los dominemos, podremos resolver cualquier misterio textual."

### üïµÔ∏è grep: El Detective de Patrones

**[DEMOSTRACI√ìN EN VIVO]**

> "grep es como tener un detective s√∫per eficiente que puede leer millones de l√≠neas por segundo buscando exactamente lo que necesitas."

```bash
# Crear archivo de ejemplo realista
cat > empleados.csv << 'EOF'
ID,Nombre,Departamento,Salario,Ciudad,Fecha_Ingreso
001,Juan P√©rez,Desarrollo,3500,Madrid,2022-01-15
002,Mar√≠a Garc√≠a,Dise√±o,3200,Barcelona,2021-03-22
003,Pedro L√≥pez,Desarrollo,4200,Valencia,2020-07-10
004,Ana Mart√≠n,Marketing,2900,Madrid,2023-02-01
005,Luis Rodr√≠guez,Dise√±o,3100,Sevilla,2021-11-30
006,Carmen S√°nchez,Desarrollo,3800,Madrid,2022-05-18
007,David Torres,Marketing,3300,Barcelona,2020-12-05
EOF
```

**[B√öSQUEDAS B√ÅSICAS]**

```bash
# Buscar desarrolladores
grep "Desarrollo" empleados.csv

# B√∫squeda insensible a may√∫sculas
grep -i "madrid" empleados.csv

# Mostrar n√∫meros de l√≠nea
grep -n "Dise√±o" empleados.csv

# Buscar l√≠neas que NO contienen el patr√≥n
grep -v "Madrid" empleados.csv
```

**[EXPLICACI√ìN DE COLORES]**

> "¬øVen c√≥mo grep resalta las coincidencias en color? Esto no es decorativo, es funcional. Los colores nos ayudan a identificar r√°pidamente lo que estamos buscando en grandes vol√∫menes de texto."

### üî¢ B√∫squedas con Patrones Num√©ricos

**[DEMOSTRACI√ìN PR√ÅCTICA]**

```bash
# Buscar salarios espec√≠ficos (usando expresiones regulares b√°sicas)
grep "3[5-9]00" empleados.csv  # Salarios entre 3500-3900

# Buscar fechas de 2022
grep "2022" empleados.csv

# Contar coincidencias
grep -c "Desarrollo" empleados.csv
sort departamentos.txt | uniq -d
```

### üìà wc: El Contador Universal

**[DEMOSTRACI√ìN PR√ÅCTICA]**

```bash
# Estad√≠sticas b√°sicas del archivo
wc empleados.csv

# Solo contar l√≠neas (empleados)
wc -l empleados.csv

# ¬øCu√°ntos empleados hay por departamento?
cut -d',' -f3 empleados.csv | sort | uniq -c

# ¬øCu√°ntos caracteres tiene el nombre m√°s largo?
cut -d',' -f2 empleados.csv | wc -L
```

### üî® cut: El Extractor Preciso

**[DEMOSTRACI√ìN EN TIEMPO REAL]**

```bash
# Extraer solo nombres
cut -d',' -f2 empleados.csv

# Extraer m√∫ltiples campos
cut -d',' -f2,4 empleados.csv

# Extraer rangos de caracteres
cut -c1-3 empleados.csv

# Combinar con otros comandos
cut -d',' -f2,4 empleados.csv | sort -t',' -k2 -n
```

### üí° Combinaciones Poderosas

**[DEMOSTRACI√ìN DE FLUJOS SIMPLES]**

```bash
# ¬øCu√°l es el salario promedio por departamento?
tail -n +2 empleados.csv | cut -d',' -f3,4 | sort

# ¬øCu√°ntos empleados √∫nicos hay en Madrid?
grep "Madrid" empleados.csv | wc -l

# Lista de departamentos √∫nicos
cut -d',' -f3 empleados.csv | tail -n +2 | sort | uniq

# El empleado con mayor salario
tail -n +2 empleados.csv | sort -t',' -k4 -nr | head -1
```

**[REFLEXI√ìN]**

> "¬øNotan c√≥mo estamos empezando a combinar comandos? Esto es solo el comienzo. En la siguiente secci√≥n aprenderemos a crear flujos de trabajo realmente potentes."

# Mostrar solo los archivos que contienen el patr√≥n

grep -l "Desarrollo" \*.csv

````markdown
### üìä sort: El Organizador Inteligente

**[PANTALLA: Demostraci√≥n visual del ordenamiento]**

> "sort no es solo ordenar alfab√©ticamente. Es una herramienta sofisticada que puede organizar datos de m√∫ltiples formas."

```bash
# Ordenamiento b√°sico
sort empleados.csv

# Ordenar por campo espec√≠fico (salario - columna 4)
sort -t',' -k4 -n empleados.csv

# Ordenar por m√∫ltiples campos: primero departamento, luego salario
sort -t',' -k3,3 -k4,4n empleados.csv

# Ordenar en reverso
sort -t',' -k4 -nr empleados.csv

# Verificar si un archivo est√° ordenado
sort -t',' -k4 -n empleados.csv -c 2>/dev/null && echo "Ordenado" || echo "No ordenado"
```

**[EXPLICACI√ìN DE PAR√ÅMETROS]**

> "Desglosemos los par√°metros:
>
> - `-t','`: usa coma como delimitador
> - `-k4`: ordena por la columna 4
> - `-n`: ordenamiento num√©rico (no alfab√©tico)
> - `-r`: orden reverso"

### üéØ uniq: El Eliminador de Duplicados

**[DEMOSTRACI√ìN INTERACTIVA]**

```bash
# Crear archivo con duplicados para demostrar
cut -d',' -f3 empleados.csv | sort > departamentos.txt
cat departamentos.txt

# Eliminar duplicados
sort departamentos.txt | uniq

# Contar ocurrencias
sort departamentos.txt | uniq -c

# Solo mostrar elementos √∫nicos (que aparecen una vez)
sort departamentos.txt | uniq -u

# Solo mostrar duplicados
sort departamentos.txt | uniq -d
```

### üìà wc: El Contador Universal

**[DEMOSTRACI√ìN PR√ÅCTICA]**

```bash
# Estad√≠sticas b√°sicas del archivo
wc empleados.csv

# Solo contar l√≠neas (empleados)
wc -l empleados.csv

# ¬øCu√°ntos empleados hay por departamento?
cut -d',' -f3 empleados.csv | sort | uniq -c

# ¬øCu√°ntos caracteres tiene el nombre m√°s largo?
cut -d',' -f2 empleados.csv | wc -L
```

### üî® cut: El Extractor Preciso

**[DEMOSTRACI√ìN EN TIEMPO REAL]**

```bash
# Extraer solo nombres
cut -d',' -f2 empleados.csv

# Extraer m√∫ltiples campos
cut -d',' -f2,4 empleados.csv

# Extraer rangos de caracteres
cut -c1-3 empleados.csv

# Combinar con otros comandos
cut -d',' -f2,4 empleados.csv | sort -t',' -k2 -n
```

### üí° Combinaciones Poderosas

**[DEMOSTRACI√ìN DE FLUJOS SIMPLES]**

```bash
# ¬øCu√°l es el salario promedio por departamento?
tail -n +2 empleados.csv | cut -d',' -f3,4 | sort

# ¬øCu√°ntos empleados √∫nicos hay en Madrid?
grep "Madrid" empleados.csv | wc -l

# Lista de departamentos √∫nicos
cut -d',' -f3 empleados.csv | tail -n +2 | sort | uniq

# El empleado con mayor salario
tail -n +2 empleados.csv | sort -t',' -k4 -nr | head -1
```

**[REFLEXI√ìN]**

> "¬øNotan c√≥mo estamos empezando a combinar comandos? Esto es solo el comienzo. En la siguiente secci√≥n aprenderemos a crear flujos de trabajo realmente potentes."

---

## üîó PARTE 3: PIPES Y FLUJOS DE DATOS (12 minutos)

### üé§ Transici√≥n

**[PANTALLA: Animaci√≥n de datos fluyendo entre comandos]**

> "Ahora llega el momento m√°gico: conectar comandos. El pipe `|` no es solo un s√≠mbolo, es una filosof√≠a. Permite que programas simples trabajen juntos para resolver problemas complejos. Es como crear una cadena de montaje digital."

### üè≠ La Filosof√≠a de la Cadena de Montaje

**[PANTALLA: Diagrama de f√°brica vs comandos Unix]**

> "Imaginen una f√°brica de autom√≥viles: cada estaci√≥n hace una tarea espec√≠fica y pasa el resultado a la siguiente. Unix funciona igual:
>
> - Estaci√≥n 1: `grep` filtra datos
> - Estaci√≥n 2: `sort` los organiza
> - Estaci√≥n 3: `uniq` elimina duplicados
> - Estaci√≥n 4: `wc` cuenta resultados"

### üîÑ Entendiendo los Flujos: stdin, stdout, stderr

**[DEMOSTRACI√ìN CONCEPTUAL]**

> "Cada programa Unix tiene tres 'tuber√≠as' de comunicaci√≥n:"

```bash
# stdout (salida est√°ndar) - descriptor 1
echo "Esto va por stdout"

# stderr (errores est√°ndar) - descriptor 2
ls archivo_inexistente

# stdin (entrada est√°ndar) - descriptor 0
# El teclado es stdin por defecto, pero podemos cambiarlo
```

**[VISUALIZACI√ìN PR√ÅCTICA]**

```bash
# Ver todos los descriptores en acci√≥n
exec 3>&1  # Guardar stdout original
exec 1>salida.txt  # Redirigir stdout a archivo
echo "Esto va al archivo"
exec 1>&3  # Restaurar stdout original
echo "Esto va a pantalla"
cat salida.txt
```

### üö∞ Pipes: Conectando la Cadena

**[DEMOSTRACI√ìN PROGRESIVA]**

> "Empezemos con pipes simples y vamos construyendo complejidad:"

```bash
# Pipe b√°sico: la salida de un comando se convierte en entrada del siguiente
ls -la | grep "txt"

# Cadena de dos pipes
ls -la | grep "txt" | wc -l

# Cadena m√°s compleja
cat empleados.csv | grep "Desarrollo" | cut -d',' -f2,4 | sort -t',' -k2 -nr
```

**[AN√ÅLISIS PASO A PASO]**

> "Analicemos este √∫ltimo comando paso a paso:
>
> 1. `cat empleados.csv`: Lee el archivo completo
> 2. `grep "Desarrollo"`: Filtra solo desarrolladores
> 3. `cut -d',' -f2,4`: Extrae nombre y salario
> 4. `sort -t',' -k2 -nr`: Ordena por salario descendente"

### üíº Casos de Uso Reales con Pipes

**[DEMOSTRACI√ìN PR√ÅCTICA EMPRESARIAL]**

```bash
# An√°lisis de logs: encontrar los errores m√°s frecuentes
echo "Simulando an√°lisis de logs..."

# Crear archivo de log simulado
cat > server.log << 'EOF'
2024-08-09 10:15:23 INFO User login successful: user123
2024-08-09 10:16:45 ERROR Database connection failed
2024-08-09 10:17:12 INFO File uploaded: document.pdf
2024-08-09 10:18:33 ERROR Database connection failed
2024-08-09 10:19:44 WARNING Memory usage high: 85%
2024-08-09 10:20:15 ERROR Authentication failed: user456
2024-08-09 10:21:32 INFO User logout: user123
2024-08-09 10:22:48 ERROR Database connection failed
2024-08-09 10:23:55 ERROR Authentication failed: user789
EOF

# An√°lisis 1: Contar tipos de mensajes
cat server.log | cut -d' ' -f4 | sort | uniq -c

# An√°lisis 2: Errores m√°s frecuentes
grep "ERROR" server.log | cut -d' ' -f5- | sort | uniq -c | sort -nr

# An√°lisis 3: Actividad por hora
cut -d' ' -f2 server.log | cut -d':' -f1 | sort | uniq -c
```

### üéØ Pipes Avanzados: tee y Process Substitution

**[DEMOSTRACI√ìN DE TEE]**

> "`tee` es como una tuber√≠a con una 'T': el flujo contin√∫a Y se guarda en un archivo"

```bash
# Guardar resultado intermedio mientras contin√∫a el procesamiento
cat empleados.csv | grep "Madrid" | tee madrid_empleados.txt | cut -d',' -f2,4

# Verificar que se guard√≥ el archivo intermedio
echo "--- Archivo guardado por tee ---"
cat madrid_empleados.txt
```

**[PROCESS SUBSTITUTION - NIVEL AVANZADO]**

```bash
# Comparar dos conjuntos de datos simult√°neamente
diff <(cut -d',' -f3 empleados.csv | sort) <(echo -e "Desarrollo\nDise√±o\nMarketing" | sort)

# Combinar m√∫ltiples fuentes
paste <(cut -d',' -f2 empleados.csv) <(cut -d',' -f4 empleados.csv) | head -5
```

### üîß Herramientas Especializadas para Pipes

**[DEMOSTRACI√ìN DE HERRAMIENTAS √öTILES]**

```bash
# head y tail: primeros y √∫ltimos elementos
cat empleados.csv | sort -t',' -k4 -nr | head -3  # Top 3 salarios

# column: formatear salida en columnas
cut -d',' -f2,3,4 empleados.csv | column -t -s','

# xargs: convertir stdin en argumentos
find . -name "*.txt" | xargs ls -la

# parallel processing con xargs
find . -name "*.csv" | xargs -P 4 -I {} wc -l {}
```

### üí° Patrones Comunes de Pipes

**[PANTALLA: Recetas de pipes √∫tiles]**

```bash
# Patr√≥n 1: An√°lisis de frecuencia
comando | sort | uniq -c | sort -nr

# Patr√≥n 2: Top N resultados
comando | sort | uniq -c | sort -nr | head -10

# Patr√≥n 3: Filtrar y contar
grep "patr√≥n" archivo | wc -l

# Patr√≥n 4: Extracci√≥n y an√°lisis
cut -d'delim' -f'campo' archivo | sort | uniq -c

# Patr√≥n 5: Limpieza y procesamiento
sed 's/patr√≥n/reemplazo/g' archivo | grep -v "^$" | sort
```

### üö® Errores Comunes y Debugging

**[DEMOSTRACI√ìN DE DEBUGGING]**

```bash
# Error com√∫n: olvidar que pipes procesan l√≠nea por l√≠nea
echo "Pipes procesan l√≠nea por l√≠nea, no archivo completo"

# Debugging: ver cada paso del pipe
cat empleados.csv | grep "Madrid" | tee /dev/stderr | cut -d',' -f2

# Usar ||  y && para manejo de errores
grep "patr√≥n" archivo.txt 2>/dev/null || echo "Patr√≥n no encontrado"

# Verificar si el pipe fall√≥
cat empleados.csv | grep "Inexistente" | wc -l
echo "Exit code del pipe completo: $?"
```

**[MEJORES PR√ÅCTICAS]**

> "Consejos para pipes efectivos:
>
> 1. Construye pipes incrementalmente - prueba cada paso
> 2. Usa `tee` para debug intermedio
> 3. Maneja errores con `2>/dev/null` cuando sea apropiado
> 4. Recuerda que pipes fallan si cualquier comando falla
> 5. El performance importa: pon filtros (`grep`) al principio"
````

## üì§ PARTE 4: REDIRECCI√ìN AVANZADA (8 minutos)

### üé§ Transici√≥n

**[PANTALLA: Diagrama de flujos de datos con flechas]**

> "Los pipes conectan comandos, pero la redirecci√≥n controla hacia d√≥nde van los datos. Es como ser el director de tr√°fico de la informaci√≥n: decides qu√© va a archivos, qu√© se descarta, y qu√© se combina."

### üóÇÔ∏è Los Tres Canales de Comunicaci√≥n

**[PANTALLA: Visualizaci√≥n de descriptores de archivo]**

> "Recordemos los tres canales b√°sicos que todo programa Unix maneja:"

```bash
# Demostraci√≥n visual de los descriptores
exec 3>&1  # Guardar stdout original en descriptor 3

echo "=== DEMOSTRACI√ìN DE DESCRIPTORES ==="
echo "Esto va por stdout (descriptor 1)" >&1
echo "Esto va por stderr (descriptor 2)" >&2
echo "Esto va por descriptor 3" >&3

exec 3>&-  # Cerrar descriptor 3
```

### üìÅ Redirecci√≥n de Salida: Controlando stdout

**[DEMOSTRACI√ìN PROGRESIVA]**

```bash
# Redirecci√≥n b√°sica - SOBRESCRIBE el archivo
echo "Primera l√≠nea" > archivo_salida.txt
echo "Segunda l√≠nea" > archivo_salida.txt  # ¬°Borra la primera!
cat archivo_salida.txt

# Redirecci√≥n append - AGREGA al archivo
echo "Primera l√≠nea" > archivo_append.txt
echo "Segunda l√≠nea" >> archivo_append.txt
echo "Tercera l√≠nea" >> archivo_append.txt
cat archivo_append.txt
```

**[CASOS PR√ÅCTICOS EMPRESARIALES]**

```bash
# Generar reportes autom√°ticos
echo "=== REPORTE DIARIO ===" > reporte_$(date +%Y%m%d).txt
echo "Fecha: $(date)" >> reporte_$(date +%Y%m%d).txt
ps aux | grep -v grep | wc -l >> reporte_$(date +%Y%m%d).txt

# Separar salida normal de errores
ls -la /home /directorio_inexistente > salida_normal.txt 2> errores.txt

# Verificar ambos archivos
echo "--- Salida Normal ---"
cat salida_normal.txt
echo "--- Errores ---"
cat errores.txt
```

### ‚ö†Ô∏è Manejo Inteligente de Errores

**[DEMOSTRACI√ìN DE STDERR]**

```bash
# Redirigir solo errores
find /etc -name "*.conf" 2> errores_find.txt

# Combinar stdout y stderr al mismo archivo
find /etc -name "*.conf" > busqueda_completa.txt 2>&1

# M√©todo alternativo (m√°s moderno)
find /etc -name "*.conf" &> busqueda_completa2.txt

# Descartar errores completamente
find /etc -name "*.conf" 2>/dev/null

# Descartar todo (stdout y stderr)
comando_ruidoso &>/dev/null
```

**[ESCENARIO REAL: SCRIPT DE BACKUP]**

```bash
# Script de backup con manejo de errores separado
cat > backup_demo.sh << 'EOF'
#!/bin/bash

BACKUP_DIR="/tmp/backup_$(date +%Y%m%d)"
LOG_FILE="/tmp/backup.log"
ERROR_FILE="/tmp/backup_errors.log"

echo "=== INICIO BACKUP $(date) ===" >> "$LOG_FILE" 2>> "$ERROR_FILE"

# Backup con logging separado
mkdir -p "$BACKUP_DIR" >> "$LOG_FILE" 2>> "$ERROR_FILE"
cp -r /home/usuario/documentos "$BACKUP_DIR/" >> "$LOG_FILE" 2>> "$ERROR_FILE"

echo "=== FIN BACKUP $(date) ===" >> "$LOG_FILE" 2>> "$ERROR_FILE"
EOF

chmod +x backup_demo.sh
# ./backup_demo.sh  # Ejecutar√≠amos en un entorno real
```

### üîÑ Redirecci√≥n de Entrada: Controlando stdin

**[DEMOSTRACI√ìN DE HERE DOCUMENTS]**

```bash
# Here document: enviar m√∫ltiples l√≠neas como input
cat << 'EOF' > configuracion.txt
servidor=localhost
puerto=8080
debug=true
timeout=30
EOF

# Here string: enviar una l√≠nea como input
grep "puerto" <<< "servidor=localhost
puerto=8080
debug=true"

# Usar archivo como input
sort < empleados.csv > empleados_ordenados.csv

# Combinaci√≥n potente: input y output redirection
sort < empleados.csv > empleados_ordenados.csv 2> errores_sort.txt
```

### üîÄ Redirecci√≥n Avanzada: Casos Complejos

**[DEMOSTRACI√ìN DE T√âCNICAS AVANZADAS]**

```bash
# Intercambiar stdout y stderr
comando 3>&1 1>&2 2>&3

# Logging completo: guardar todo pero tambi√©n ver en pantalla
ls -la /home /directorio_inexistente 2>&1 | tee log_completo.txt

# Pipeline con redirecci√≥n compleja
cat empleados.csv | grep "Madrid" | sort -t',' -k4 -nr | tee madrid_sorted.txt | head -3

# Redirecci√≥n condicional basada en √©xito/fallo
if grep "ERROR" server.log > errores_encontrados.txt 2>/dev/null; then
    echo "Se encontraron errores. Revisa errores_encontrados.txt"
else
    echo "No hay errores en el log"
fi
```

### üìä Process Substitution: Lo M√°s Avanzado

**[DEMOSTRACI√ìN DE PROCESS SUBSTITUTION]**

```bash
# Comparar salidas de dos comandos sin archivos temporales
diff <(sort empleados.csv) <(sort -r empleados.csv)

# Usar m√∫ltiples inputs
paste <(cut -d',' -f2 empleados.csv) <(cut -d',' -f4 empleados.csv) <(cut -d',' -f5 empleados.csv)

# Combinar con pipes normales
cat <(echo "=== ENCABEZADO ===") empleados.csv <(echo "=== PIE ===") | head -10
```

### üîß Named Pipes (FIFOs) - Nivel Experto

**[DEMOSTRACI√ìN CONCEPTUAL]**

```bash
# Crear named pipe
mkfifo mi_pipe

# En una terminal (simularemos con comentario):
# echo "Datos por pipe" > mi_pipe &

# En otra terminal:
# cat < mi_pipe

# Limpiar
rm mi_pipe
```

### üí° Patrones √ötiles de Redirecci√≥n

**[PANTALLA: Recetario de redirecci√≥n]**

```bash
# Patr√≥n 1: Log completo con timestamp
comando 2>&1 | while read line; do echo "$(date): $line"; done >> log_con_tiempo.txt

# Patr√≥n 2: Backup antes de sobrescribir
[ -f archivo.txt ] && cp archivo.txt archivo.txt.bak
comando > archivo.txt

# Patr√≥n 3: Capturar y mostrar errores
comando 2> >(while read line; do echo "ERROR: $line" >&2; done)

# Patr√≥n 4: Multiplex output
comando | tee archivo1.txt archivo2.txt archivo3.txt

# Patr√≥n 5: Silent execution con logging
{ comando 2>&1; echo "Exit code: $?"; } >> log_completo.txt
```

### üö® Errores Comunes en Redirecci√≥n

**[DEMOSTRACI√ìN DE ERRORES T√çPICOS]**

```bash
# ERROR: No se puede leer y escribir al mismo archivo
# grep "patr√≥n" archivo.txt > archivo.txt  # ¬°Esto borra el archivo!

# CORRECTO: Usar archivo temporal
grep "patr√≥n" archivo.txt > temp.txt && mv temp.txt archivo.txt

# ERROR: Orden incorrecto de redirecci√≥n
# comando 2>&1 > archivo.txt  # stderr va a pantalla, no al archivo

# CORRECTO:
comando > archivo.txt 2>&1  # Ambos van al archivo

# ERROR: Olvidar que > borra el archivo
echo "l√≠nea 1" > archivo.txt
echo "l√≠nea 2" > archivo.txt  # ¬°l√≠nea 1 se perdi√≥!

# CORRECTO:
echo "l√≠nea 1" > archivo.txt
echo "l√≠nea 2" >> archivo.txt
```

**[MEJORES PR√ÅCTICAS]**

> "Reglas de oro para redirecci√≥n:
>
> 1. Usa `>>` para agregar, `>` solo cuando quieras sobrescribir
> 2. Siempre maneja stderr expl√≠citamente en scripts
> 3. Usa `tee` cuando necesites ver y guardar simult√°neamente
> 4. Prueba redirecciones con archivos de prueba primero
> 5. Recuerda que `2>&1` debe ir AL FINAL para capturar todo"

````

## üéØ PARTE 5: EXPRESIONES REGULARES Y AWK (15 minutos)

### üé§ Transici√≥n

**[PANTALLA: Patr√≥n de regex animado]**

> "Ahora llegamos a las herramientas m√°s poderosas del procesamiento de texto: las expresiones regulares y AWK. Si grep es un detective, las expresiones regulares son su lupa de alta precisi√≥n. Y si AWK es un analista, es uno que puede programar mientras analiza."

### üîç Expresiones Regulares: El Lenguaje de los Patrones

**[PANTALLA: Introducci√≥n visual a regex]**

> "Las expresiones regulares (regex) son un lenguaje para describir patrones de texto. Es como tener superpoderes para buscar: en lugar de buscar texto exacto, describes C√ìMO debe verse el texto que buscas."

### üìù Metacaracteres B√°sicos

**[DEMOSTRACI√ìN PROGRESIVA]**

```bash
# Crear archivo de prueba para regex
cat > datos_regex.txt << 'EOF'
juan.perez@empresa.com
maria_garcia@gmail.com
pedro123@hotmail.es
ana.martin@universidad.edu
luis@dominio.co.uk
carmen.sanchez@empresa.com
david.torres@outlook.com
123-456-7890
+34-91-123-4567
(555) 123-4567
usuario@dominio
email_sin_punto@empresa
EOF

# Metacar√°cter . (cualquier car√°cter)
grep "p.dro" datos_regex.txt

# Metacar√°cter * (cero o m√°s repeticiones)
grep "pe.*ez" datos_regex.txt

# Metacar√°cter + (una o m√°s repeticiones)
grep -E "pe.+ez" datos_regex.txt

# Metacar√°cter ? (cero o una repetici√≥n)
grep -E "colou?r" datos_regex.txt
```

### üéØ Anclas y Posiciones

**[DEMOSTRACI√ìN DE ANCLAS]**

```bash
# ^ inicio de l√≠nea
grep "^maria" datos_regex.txt

# $ final de l√≠nea
grep "\.com$" datos_regex.txt

# \b l√≠mite de palabra
grep -E "\bana\b" datos_regex.txt

# Combinaci√≥n: emails que empiecen y terminen con condiciones espec√≠ficas
grep -E "^[a-z]+.*\.com$" datos_regex.txt
```

### üìö Clases de Caracteres

**[DEMOSTRACI√ìN INTERACTIVA]**

```bash
# [abc] uno de los caracteres listados
grep "[aei]@" datos_regex.txt

# [a-z] rango de caracteres
grep "[a-z]@[a-z]" datos_regex.txt

# [^abc] cualquier car√°cter EXCEPTO los listados
grep "[^a-z]@" datos_regex.txt

# Clases predefinidas con grep -E
grep -E "[[:digit:]]+" datos_regex.txt    # n√∫meros
grep -E "[[:alpha:]]+" datos_regex.txt    # letras
grep -E "[[:alnum:]]+" datos_regex.txt    # letras y n√∫meros
```

### üìû Caso Pr√°ctico: Validaci√≥n de Tel√©fonos

**[DEMOSTRACI√ìN REAL]**

```bash
# Tel√©fonos en formato XXX-XXX-XXXX
grep -E "[0-9]{3}-[0-9]{3}-[0-9]{4}" datos_regex.txt

# Tel√©fonos internacionales con +
grep -E "\+[0-9]{2}-[0-9]{2}-[0-9]{3}-[0-9]{4}" datos_regex.txt

# Tel√©fonos con par√©ntesis
grep -E "\([0-9]{3}\) [0-9]{3}-[0-9]{4}" datos_regex.txt

# Cualquier formato de tel√©fono (regex compleja)
grep -E "(\+[0-9]{2}-[0-9]{2}-[0-9]{3}-[0-9]{4})|([0-9]{3}-[0-9]{3}-[0-9]{4})|(\([0-9]{3}\) [0-9]{3}-[0-9]{4})" datos_regex.txt
```

### üìß Validaci√≥n de Emails con Regex

**[DEMOSTRACI√ìN AVANZADA]**

```bash
# Email b√°sico: caracteres @ caracteres . caracteres
grep -E "[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}" datos_regex.txt

# Emails de empresa espec√≠fica
grep -E "[a-zA-Z0-9._-]+@empresa\.com" datos_regex.txt

# Emails con dominios universitarios
grep -E "[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\.edu" datos_regex.txt

# Validar estructura de email completa
grep -E "^[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$" datos_regex.txt
```

### ‚öôÔ∏è sed con Expresiones Regulares

**[DEMOSTRACI√ìN DE TRANSFORMACIONES]**

```bash
# Ocultar parte del email (privacidad)
sed -E 's/([a-zA-Z0-9._-]+)@([a-zA-Z0-9.-]+)/***@\2/g' datos_regex.txt

# Formatear tel√©fonos a un est√°ndar
sed -E 's/\+([0-9]{2})-([0-9]{2})-([0-9]{3})-([0-9]{4})/(\1) \2-\3-\4/g' datos_regex.txt

# Extraer dominio de emails
sed -E 's/.*@([a-zA-Z0-9.-]+\.[a-zA-Z]{2,}).*/\1/g' datos_regex.txt

# Validar y limpiar datos
sed -E '/^[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$/!d' datos_regex.txt
```

---

## üîß AWK: El Procesador Inteligente

### üé§ Introducci√≥n a AWK

**[PANTALLA: Logo AWK con ejemplos]**

> "AWK no es solo un comando, es un lenguaje de programaci√≥n completo dise√±ado para procesamiento de texto. Fue creado por Aho, Weinberger y Kernighan (de ah√≠ AWK) y es perfecto para datos estructurados."

### üìä Conceptos Fundamentales de AWK

**[DEMOSTRACI√ìN CONCEPTUAL]**

```bash
# Archivo de datos para AWK
cat > ventas.csv << 'EOF'
Fecha,Vendedor,Producto,Cantidad,Precio
2024-01-15,Juan,Laptop,2,1500
2024-01-16,Mar√≠a,Mouse,10,25
2024-01-17,Pedro,Teclado,5,80
2024-01-18,Ana,Monitor,3,300
2024-01-19,Luis,Laptop,1,1500
2024-01-20,Carmen,Mouse,15,25
2024-01-21,David,Teclado,8,80
EOF

# Estructura b√°sica de AWK: patr√≥n { acci√≥n }
awk -F',' '{print $2, $3}' ventas.csv  # Vendedor y producto

# AWK procesa l√≠nea por l√≠nea autom√°ticamente
awk -F',' 'NR > 1 {print "Vendedor:", $2, "vendi√≥", $4, $3}' ventas.csv
```

### üî¢ Variables Autom√°ticas de AWK

**[DEMOSTRACI√ìN DE VARIABLES]**

```bash
# NR: n√∫mero de registro (l√≠nea)
awk -F',' '{print "L√≠nea", NR ":", $0}' ventas.csv

# NF: n√∫mero de campos en la l√≠nea actual
awk -F',' '{print "L√≠nea con", NF, "campos:", $0}' ventas.csv

# $0: l√≠nea completa, $1, $2, etc.: campos espec√≠ficos
awk -F',' '{print "Campo 1:", $1, "- Campo 2:", $2}' ventas.csv

# FS: separador de campos (equivalente a -F)
awk 'BEGIN{FS=","} {print $2, $3}' ventas.csv
```

### üí∞ C√°lculos y Agregaciones

**[DEMOSTRACI√ìN DE AN√ÅLISIS FINANCIERO]**

```bash
# Calcular total de ventas por producto
awk -F',' 'NR > 1 {
    total = $4 * $5;
    print $3 ":", "$" total;
}' ventas.csv

# Sumar total de ventas
awk -F',' 'NR > 1 {
    total += $4 * $5;
}
END {
    print "Total de ventas: $" total;
}' ventas.csv

# Estad√≠sticas por vendedor
awk -F',' 'NR > 1 {
    vendedor = $2;
    ventas[vendedor] += $4 * $5;
    count[vendedor]++;
}
END {
    print "=== ESTAD√çSTICAS POR VENDEDOR ===";
    for (v in ventas) {
        printf "%-10s: $%-8.2f (%d ventas)\n", v, ventas[v], count[v];
    }
}' ventas.csv
```

### üéØ Filtros Condicionales en AWK

**[DEMOSTRACI√ìN DE CONDICIONES]**

```bash
# Ventas superiores a $200
awk -F',' 'NR > 1 && $4 * $5 > 200 {
    printf "VENTA GRANDE: %s vendi√≥ %d %s por $%.2f\n", $2, $4, $3, $4*$5;
}' ventas.csv

# Vendedores espec√≠ficos
awk -F',' '$2 == "Juan" || $2 == "Mar√≠a" {print $0}' ventas.csv

# Productos con alta cantidad
awk -F',' 'NR > 1 && $4 >= 5 {
    print $2 " vendi√≥ muchas unidades de " $3 " (" $4 " unidades)";
}' ventas.csv
```

### üìà Reportes Avanzados con AWK

**[DEMOSTRACI√ìN DE REPORTE COMPLETO]**

```bash
# Reporte ejecutivo completo
awk -F',' '
BEGIN {
    print "========================================";
    print "         REPORTE DE VENTAS";
    print "         Generado: " strftime("%Y-%m-%d %H:%M:%S");
    print "========================================\n";
}

{
    # Extraer campos del log
    ip = $1;
    timestamp = $4;
    gsub(/\[/, "", timestamp);
    method = $6;
    gsub(/"/, "", method);
    url = $7;
    status = $9;
    bytes = $10;

    # Contadores generales
    total_requests++;
    total_bytes += bytes;

    # Por IP
    requests_by_ip[ip]++;
    bytes_by_ip[ip] += bytes;

    # Por c√≥digo de estado
    status_codes[status]++;

    # Por m√©todo HTTP
    methods[method]++;

    # Por URL
    urls[url]++;

    # Detectar patrones sospechosos
    if (status == 401) {
        unauthorized[ip]++;
    }
    if (url ~ /admin|login|wp-admin/) {
        sensitive_access[ip]++;
    }
}

END {
    # Resumen general
    printf "RESUMEN GENERAL:\n";
    printf "  Total de requests: %d\n", total_requests;
    printf "  Total de bytes transferidos: %.2f MB\n", total_bytes/1024/1024;
    printf "  Promedio de bytes por request: %.0f\n", total_bytes/total_requests;
    print "";

    # Top IPs por tr√°fico
    printf "TOP 5 IPs POR TR√ÅFICO:\n";
    n = asorti(bytes_by_ip, sorted_ips);
    for (i = n; i >= n-4 && i >= 1; i--) {
        ip = sorted_ips[i];
        printf "  %-15s: %d requests, %.2f MB\n",
               ip, requests_by_ip[ip], bytes_by_ip[ip]/1024/1024;
    }
    print "";

    # C√≥digos de estado
    printf "DISTRIBUCI√ìN DE C√ìDIGOS DE ESTADO:\n";
    for (code in status_codes) {
        printf "  %s: %d (%.1f%%)\n",
               code, status_codes[code],
               (status_codes[code]/total_requests)*100;
    }
    print "";

    # Alertas de seguridad
    printf "ALERTAS DE SEGURIDAD:\n";
    alert_count = 0;
    for (ip in unauthorized) {
        if (unauthorized[ip] >= 3) {
            printf "  ALERTA: IP %s tuvo %d errores 401\n", ip, unauthorized[ip];
            alert_count++;
        }
    }
    for (ip in sensitive_access) {
        if (sensitive_access[ip] >= 2) {
            printf "  ALERTA: IP %s accedi√≥ %d veces a URLs sensibles\n",
                   ip, sensitive_access[ip];
            alert_count++;
        }
    }
    if (alert_count == 0) {
        printf "  No se detectaron patrones sospechosos.\n";
    }

    print "\n========================================";
}' "$ACCESS_LOG" > "$REPORT_DIR/reporte_completo.txt"

# Mostrar reporte en pantalla
cat "$REPORT_DIR/reporte_completo.txt"
EOF
```

### üìß M√≥dulo 5: Sistema de Alertas

**[DEMOSTRACI√ìN: Finalizando el proyecto]**

```bash
# Agregar sistema de alertas y finalizaci√≥n
cat >> analizador.sh << 'EOF'

# === SISTEMA DE ALERTAS ===
echo ""
echo -e "${YELLOW}=== VERIFICANDO ALERTAS CR√çTICAS ===${NC}"

# Funci√≥n para enviar alerta (simulada)
send_alert() {
    local level="$1"
    local message="$2"
    echo -e "${RED}[ALERTA $level] $message${NC}"
    echo "[$(date)] [ALERTA $level] $message" >> "$REPORT_DIR/alertas.log"

    # En un entorno real, aqu√≠ enviar√≠as email, slack, etc.
    # mail -s "Alerta de Seguridad" admin@empresa.com < alerta.txt
    # curl -X POST slack_webhook -d "{\"text\":\"$message\"}"
}

# Verificar alertas cr√≠ticas
critical_ips=$(awk '$9 == 401 {print $1}' "$ACCESS_LOG" | sort | uniq -c | \
               awk -v threshold=5 '$1 >= threshold {print $2}' | wc -l)

if [ "$critical_ips" -gt 0 ]; then
    send_alert "CR√çTICA" "Se detectaron $critical_ips IPs con m√∫ltiples fallos de autenticaci√≥n"
fi

# Verificar errores 500
server_errors=$(awk '$9 == 500' "$ACCESS_LOG" | wc -l)
if [ "$server_errors" -gt 0 ]; then
    send_alert "ALTA" "Se detectaron $server_errors errores internos del servidor"
fi

# === FINALIZACI√ìN ===
echo ""
echo -e "${GREEN}=== AN√ÅLISIS COMPLETADO ===${NC}"
echo "Reportes guardados en: $REPORT_DIR/"
echo "  - reporte_completo.txt: An√°lisis detallado"
echo "  - ips_sospechosas.txt: IPs con actividad sospechosa"
echo "  - alertas.log: Log de alertas generadas"

# Crear script de monitoreo continuo
cat > monitor_continuo.sh << 'MONITOR_EOF'
#!/bin/bash
# Monitor continuo de logs

while true; do
    echo "$(date): Ejecutando an√°lisis de logs..."
    ./analizador.sh
    echo "Pr√≥ximo an√°lisis en 5 minutos..."
    sleep 300
done
MONITOR_EOF

chmod +x monitor_continuo.sh

echo ""
echo -e "${BLUE}Scripts adicionales creados:${NC}"
echo "  - monitor_continuo.sh: Para monitoreo en tiempo real"
echo ""
echo -e "${GREEN}¬°Analizador de logs completado con √©xito!${NC}"
EOF
```

### üß™ Probando el Analizador Completo

**[DEMOSTRACI√ìN EN VIVO]**

```bash
# Ejecutar el analizador completo
echo -e "\n${BLUE}=== EJECUTANDO EL ANALIZADOR COMPLETO ===${NC}"
./analizador.sh

# Verificar los archivos generados
echo -e "\n${GREEN}=== ARCHIVOS GENERADOS ===${NC}"
ls -la reportes_*/

# Mostrar resumen del reporte
echo -e "\n${YELLOW}=== RESUMEN DEL REPORTE ===${NC}"
head -20 reportes_*/reporte_completo.txt
```

### üéì Lo que Hemos Logrado

**[PANTALLA: Resumen de logros]**

> "¬°Incre√≠ble! En este m√≥dulo hemos creado un analizador de logs profesional que:
> ‚úÖ **Procesa** logs de servidor web en formato est√°ndar
> ‚úÖ **Analiza** tr√°fico y detecta patrones
> ‚úÖ **Identifica** amenazas de seguridad
> ‚úÖ **Genera** reportes autom√°ticos
> ‚úÖ **Crea** alertas inteligentes
> ‚úÖ **Combina** todas las herramientas del m√≥dulo 2"

### üöÄ Pr√≥ximos Pasos y M√≥dulo 3

**[PANTALLA: Roadmap futuro]**

> "Este analizador es solo el comienzo. En el **M√≥dulo 3: Variables y Control de Flujo** aprenderemos:
>
> - **Variables avanzadas** y arrays
> - **Estructuras de control** (if, for, while, case)
> - **Funciones personalizadas** para modularizar c√≥digo
> - **Manejo de argumentos** y configuraciones
> - **Validaci√≥n de datos** robusta
> - **Scripts interactivos** con men√∫s"

### üìö Recursos para Continuar

**[PANTALLA: Desaf√≠os adicionales]**

> "Para seguir practicando:
>
> **Desaf√≠os:**
> - Agrega soporte para logs de NGINX
> - Implementa alertas por email/Slack reales
> - Crea dashboard web con los datos
> - A√±ade an√°lisis de geolocalizaci√≥n de IPs
>
> **Lecturas:**
> - Manual de Apache/Nginx log formats
> - Gu√≠a de expresiones regulares avanzadas
> - Best practices de monitoreo de sistemas"

**[MENSAJE FINAL]**

> "¬°Felicitaciones! Has dominado el procesamiento de texto avanzado en bash. Estas habilidades te convertir√°n en un administrador de sistemas y desarrollador m√°s eficiente. ¬°Nos vemos en el M√≥dulo 3 para dar el siguiente gran paso!"
````
